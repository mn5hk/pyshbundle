{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to PySHbundle","text":"<p>PySHbundle: A Python implementation of MATLAB codes SHbundle</p> <p>Our package, <code>PySHbundle</code> has been developed in a modularized manner. The package provides tools to process GRACE data, such as, the computation of anomalies, substitution of poor quality low degree coefficients, reducing noise in GRACE data using filtering approaches, signal leakage correction using <code>GDDC</code>, etc. In addition, the package provides a flexibility for future development and addition of further processing choices for handling GRACE data for hydrological application.</p> <p>It is hoped the contribution will make GRACE L2 data processing more accessible to a wider audience of young researchers. Our python package is titled <code>PySHbundle</code> and the working code can be accessed at GitHub. </p>"},{"location":"#how-to-install","title":"How to install","text":"<p><pre><code># creating a new virtual environment\n$ python3 -m venv &lt;name-env&gt;\n# activate the virtual environment environment\n$ source &lt;/location-of-virt-env/name-env/bin/activate&gt;\n# install package into virtual environment\n$ pip install pyshbundle\n\n# clone the repository in order to access the notebooks and data\n$ git clone git@github.com:mn5hk/pyshbundle.git\n</code></pre> For more details refer to the installation section.</p>"},{"location":"#group","title":"Group","text":"<p>Geodesy for Earth system science (GESS) research Group at ICWaR, IISc</p>"},{"location":"#credits-for-the-theme","title":"Credits for the theme","text":"<p>This package was created with Cookiecutter and the giswqs/pypackage project template.</p>"},{"location":"acknowledgement/","title":"Acknowledgement:","text":"<p>Please note that PySHbundle has adapted the following code packages, both licensed under GNU General Public License</p> <ol> <li> <p>SHbundle: https://www.gis.uni-stuttgart.de/en/research/downloads/shbundle/</p> </li> <li> <p>Downscaling GRACE Total Water Storage Change using Partial Least Squares Regression - https://springernature.figshare.com/collections/Downscaling_GRACE_Total_Water_Storage_Change_using_Partial_Least_Squares_Regression/5054564 </p> </li> </ol>"},{"location":"acknowledgement/#key-papers-referred","title":"Key Papers Referred:","text":"<ol> <li> <p>Vishwakarma, B. D., Horwath, M., Devaraju, B., Groh, A., &amp; Sneeuw, N. (2017).        A data\u2010driven approach for repairing the hydrological catchment signal damage        due to filtering of GRACE products. Water Resources Research,        53(11), 9824-9844. https://doi.org/10.1002/2017WR021150</p> </li> <li> <p>Vishwakarma, B. D., Zhang, J., &amp; Sneeuw, N. (2021).        Downscaling GRACE total water storage change using        partial least squares regression. Scientific data, 8(1), 95.       https://doi.org/10.1038/s41597-021-00862-6</p> </li> </ol>"},{"location":"auxillary_codes/","title":"Auxillary Codes","text":"<p>The rest of the important functions have been bundled under the <code>auxillary codes</code> module. </p>"},{"location":"auxillary_codes/#computing-legendre-functions","title":"Computing Legendre functions","text":"<p>plm Fully normalized associated Legendre functions for a selected order M</p> <p>iplm Integrals of the fully normalized associated Legendre functions     over blocks for a selected order M. </p>"},{"location":"auxillary_codes/#pyshbundle.shutils.plm--parameters","title":"Parameters","text":"<p>l : numpy.array     Degree, but not necessarily monotonic.         For l &lt; m a vector of zeros will be returned. m : int     order. If absent, m = 0 is assumed. thetaRAD : numpy.array     Co-latitude in radians nargin : int     number of input argument nargout : int     number of output argument Returns</p> <p>p : array     plm fully normalized Legendre functions dp : array     first derivative of plm ddp : array     second derivative of plm</p> Author <p>Vivek Kumar Yadav, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/shutils.py</code> <pre><code>def plm(l: np.array, m:int, thetaRAD, nargin, nargout): \n    \"\"\"plm Fully normalized associated Legendre functions for a selected order M\n\n    Parameters\n    ----------\n    l : numpy.array\n        Degree, but not necessarily monotonic.\n            For l &lt; m a vector of zeros will be returned.\n    m : int\n        order. If absent, m = 0 is assumed.\n    thetaRAD : numpy.array\n        Co-latitude in radians\n    nargin : int\n        number of input argument\n    nargout : int\n        number of output argument\n    Returns\n    ----------\n    p : array\n        plm fully normalized Legendre functions\n    dp : array\n        first derivative of plm\n    ddp : array\n        second derivative of plm\n\n    Author:\n        Vivek Kumar Yadav, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n\n    if  min(l.shape) != 1:\n        raise ValueError('Degree l must be a vector (or scalar)') \n    if  np.remainder(l,1).all() != 0:\n        raise ValueError('Vector l contains non-integers !!') \n    if  np.remainder(m,1) != 0:\n        raise ValueError('Order must be integer')\n\n# PRELIMINARIES\n    lcol = len(l)\n    trow = len(thetaRAD)\n    lmax = int(max(l[0,:]))\n\n    if lmax &lt; m:\n        p = np.zeros([len(thetaRAD), len(l)], dtype='float')\n        dp = np.zeros([len(thetaRAD), len(l)], dtype='float')\n        ddp = np.zeros([len(thetaRAD), len(l)], dtype='float')\n        sys.exit([])\n\n    n = thetaRAD.size                                                               # number of latitudes\n    t = thetaRAD[:]\n    x = np.cos(t)\n    y = np.sin(t)\n    lvec = np.transpose(l)   \n    lvec = np.intc(lvec)                                                       # l can now be used as running index\n\n    if min(t).all() &lt; 0 and max(t).all() &gt; np.pi:\n        print('Warning: Is co-latitude in radians ?')\n\n    # Recursive computation of the temporary matrix ptmp, containing the Legendre\n    # functions in its columns, with progressing degree l. The last column of\n    # ptmp will contain zeros, which is useful for assignments when l &lt; m.\n    ptmp = np.zeros((n,lmax + 2 - m))\n    if nargout &gt;= 2:                                                                #  first derivative needs also P_{n,m+1} and P_{n,m-1}\n        ptmp_m1 = np.zeros((n,lmax + 3 - m), dtype='float')\n        ptmp_p1 = np.zeros((n,lmax + 1 -m), dtype='float')        \n        dptmp = np.zeros((n,lmax + 2 - m), dtype='float') \n    if nargout == 3:                                                                # second derivative needs also dP_{n,m+1} and dP_{n,m-1}\n        dptmp_m1 = np.zeros((n,lmax + 3 -m), dtype='float')\n        dptmp_p1 = np.zeros((n,lmax + 1 -m), dtype='float')\n        ptmp_m2 = np.zeros((n,lmax + 4 -m), dtype='float')                                         # but these first derivative need dP_{n,m+2} and dP_{n,m-2}\n        ptmp_p2 = np.zeros((n,lmax - m), dtype='float')\n        ddptmp = np.zeros((n,lmax + 2 -m), dtype='float')\n\n    # sectorial recursion: PM (non-recursive, though)\n    ptmp[:,0] = secrecur(m,y)\n    if nargout &gt;= 2:                                                                # frist derivative needs preceding and subsequent element\n        if m &gt; 0:    \n            ptmp_m1[:,0] = secrecur(m-1,y)                                          # preceding elements\n        if m &lt; lmax: \n            ptmp_p1[:,0] = secrecur(m+1,y)                                          # subsequent elemtens\n    if nargout == 3:                                                                # second derivative needs P_{n,m+2} and P_{n,m-2} as well\n        if m &gt; 1:           \n            ptmp_m2[:,0] = secrecur(m-2,y)                                          # preceding elements\n        if m &lt; lmax-1: \n            ptmp_p2[:,0] = secrecur(m+2,y)                                          # subsequent elemtens\n\n    # l-recursion: P\n    ptmp = lrecur(ptmp,x,m,lmax);\n    if nargout &gt;= 2:                                                                # frist derivative needs preceding and subsequent element\n        if m &gt; 0:\n            ptmp_m1 = lrecur(ptmp_m1,x,m-1,lmax)                                    # preceding elements\n        if m &lt; lmax:\n            ptmp_p1 = lrecur(ptmp_p1,x,m+1,lmax)                                    # subsequent elemtens\n\n    if nargout == 3:                                                                # second derivative needs P_{n,m+2} and P_{n,m-2} as well\n        if m &gt; 1:\n            ptmp_m2 = lrecur(ptmp_m2,x,m-2,lmax)                                    # preceding elements\n        if m &lt; lmax-1:\n            ptmp_p2 = lrecur(ptmp_p2,x,m+2,lmax)                                    # subsequent elemtens\n\n    # now compute the derivatives \n    if nargout &gt;= 2:                                                                # first derivative\n        dptmp = derivALF(dptmp,ptmp_m1,ptmp_p1,m,lmax)\n    if nargout == 3:                                                                # second derivative\n        if m &gt; 0:    \n            dptmp_m1 = derivALF(dptmp_m1,ptmp,ptmp_m2,m-1,lmax)\n        if m &lt; lmax:\n            dptmp_p1 = derivALF(dptmp_p1,ptmp,ptmp_p2,m+1,lmax)\n        ddptmp = derivALF(ddptmp,dptmp_m1,dptmp_p1,m,lmax)\n\n\n# --------------------------------------------------------------------\n        # The Legendre functions have been computed. What remains to be done, is to\n        # extract the proper columns from ptmp, corresponding to the vector lvec. \n        # If l or thetaRAD is scalar the output matrix p reduces to a vector. It should\n        # have the shape of respectively thetaRAD or l in that case.\n# --------------------------------------------------------------------\n    lind       = (lvec &lt; m)   \t # index into l &lt; m\n    pcol       = lvec - m + 0\t\t\t                                            # index into columns of ptmp\n    pcol[lind] = np.ndarray((lmax-m+2-6)*np.ones((sum(sum(lind)),1)))\t            # Now l &lt; m points to last col.\n    p      = ptmp[:,pcol]\t\t\t                                                # proper column extraction \n    if nargout &gt;= 2:\n        dp =  dptmp[:,pcol]                                                         # proper column extraction \n    if nargout == 3: \n        ddp = ddptmp[:,pcol]                                                        # proper column extraction  \n    if max(lvec.shape)==1  and min(thetaRAD.shape)==1 and (trow == 1):\n        p = p.T\n        if nargout &gt;= 2:\n            dp  = np.transpose(dp)\n        if nargout == 3:\n            ddp = np.transpose(ddp)\n    if max(thetaRAD.shape)==1 and min(lvec.shape)==1  and (lcol == 1):\n        p = p.T\n        if nargout &gt;= 2:\n            dp  = dp.T  \n        if nargout == 3:\n            ddp = ddp.T\n\n    if nargout == 1: \n        return p\n    if nargout == 2: \n        return p,dp\n    if nargout == 3: \n        return p, dp, ddp\n</code></pre>"},{"location":"auxillary_codes/#pyshbundle.shutils.iplm--parameters","title":"Parameters","text":"<p>l : numpy.array     degree (vector). Integer, but not necessarily monotonic.     For l &lt; m a vector of zeros will be returned. m : int     Order of the Legendre function. If absent, m = 0 is assumed. theRAD : numpy.array      co-latitude in radian dt : (int, optional)     integration block-size [rad] (scalar). Defaults to -9999.</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.iplm--returns","title":"Returns","text":"<p>numpy.ndarray     Matrix with integrated Legendre functions.     Functions are integrated from theRAD(i)-dt/2 till theRAD(i)+dt/2.     The matrix has length(TH) rows and length(L) columns, unless L      or TH is scalar. Then the output vector follows the shape of      respectively L or TH.</p> Notes <p>The blocks at the pole might become too large under circumstances. This is not treated separately, i.e. unwanted output may appear. In case TH is scalar, dt will be 1 (arbitrarily).</p> Uses <p><code>plm</code></p> Author <p>Vivek Kumar Yadav, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/shutils.py</code> <pre><code>def iplm(l, m:int, theRAD, dt=-9999):\n    \"\"\"iplm Integrals of the fully normalized associated Legendre functions\n        over blocks for a selected order M. \n\n    Parameters\n    ----------\n    l : numpy.array\n        degree (vector). Integer, but not necessarily monotonic.\n        For l &lt; m a vector of zeros will be returned.\n    m : int\n        Order of the Legendre function. If absent, m = 0 is assumed.\n    theRAD : numpy.array \n        co-latitude in radian\n    dt : (int, optional)\n        integration block-size [rad] (scalar). Defaults to -9999.\n\n    Returns\n    ----------\n    numpy.ndarray\n        Matrix with integrated Legendre functions.\n        Functions are integrated from theRAD(i)-dt/2 till theRAD(i)+dt/2.\n        The matrix has length(TH) rows and length(L) columns, unless L \n        or TH is scalar. Then the output vector follows the shape of \n        respectively L or TH.\n\n    Notes:\n        The blocks at the pole might become too large under circumstances.\n        This is not treated separately, i.e. unwanted output may appear.\n        In case TH is scalar, dt will be 1 (arbitrarily).\n\n    Uses:\n        `plm`\n\n    Author:\n        Vivek Kumar Yadav, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n\n    if dt == -9999:\n        dt = np.pi / 180 if len(theRAD) == 1 else theRAD[1] - theRAD[0]\n\n    if min(l.shape) != 1:\n        raise ValueError('Degree l must be a vector (or scalar)')\n\n    if not np.all(np.remainder(l, 1) == 0):\n        raise ValueError('Vector l contains non-integers !!')\n\n    if not np.remainder(m, 1) == 0:\n        raise ValueError('Order must be integer')\n\n    if dt == 0:\n        raise ValueError('DT cannot be zero')\n\n\n    lcol = len(l)\n    trow = len(theRAD)\n    n = len(theRAD)\n    theRAD.T\n    if min(theRAD) &lt; 0 or max(theRAD) &gt; np.pi:\n        raise ValueError('Is the co-latitude ''theta'' given in radian?')\n\n    lmax = max(l[0])\n    mfix = m\n    lvec = np.transpose(l) \n    l = np.arange(mfix,lmax+1,1)\n\n    # Initialization of cosine, sine and Plm functions\n    stplus  = np.sin(theRAD+dt/2)\n    stmin   = np.sin(theRAD-dt/2)\n    ctplus  = np.cos(theRAD+dt/2)\n    ctmin   = np.cos(theRAD-dt/2)\n    plmplus = np.ones([n,lmax+1])\n    plmmin = np.ones([n,lmax + 1])\n    plmplus[:,l] = plm(np.array([l]),mfix,(theRAD + dt/2),3,1)[:,:,0]                  # Tesserals\n    plmmin[:,l] = plm(np.array([l]),mfix,(theRAD - dt/2),3,1)[:,:,0] \n    if mfix &gt; 0:\n        m = np.arange(1,mfix + 1,1)\n        mm = 2*m\n        fac = np.sqrt(2*np.cumprod((mm+1)/mm))\n        mgr, stp = np.meshgrid(m, stplus)\n        fgr, stm = np.meshgrid(fac, stmin)\n        plmplus[:, m] = fgr * np.power(stp, mgr)\n        plmmin[:, m] = fgr * np.power(stm, mgr)\n    ptmp = np.zeros([n, lmax +2 ])\n    ptmp00 = np.cos(theRAD - dt/2) - ctplus\n    ptmp11 = np.sqrt(3)/2 * (dt - ctplus* stplus + ctmin* stmin)\n    ptmp10 = np.sqrt(3)/2 * (np.power(stplus,2) - np.power(stmin,2))\n    ptmp[:,0] = ptmp00\n\n    # Compute first the integrals of order m == 0\n    if mfix == 0:\n        ptmp[:,1] = ptmp10\n        for l in range(2,lmax+1,1):              #loop over the degree l \n            rootnm = np.sqrt( (2*l+1)*(2*l-1)/np.power(l,2))\n            root1nm = np.sqrt( (2*l-1)*(2*l-3)/np.power(l-1,2))\n            ptmp[:,l] = rootnm/(l+1)*(((l-2)*ptmp[:,l-2]/root1nm).T + np.power(stplus,2)*plmplus[:,l-1].T - np.power(stmin,2)*plmmin[:,l-1].T )\n    else:\n        # Compute the integrals of order m &gt; 0\n\n        # First we compute the diagonal element IPmm (lmax == mfix)\n\n        ptmp[:,1] = ptmp11\n        for l in range(2,mfix+1,1):\n            # print(l)\n            rootmm = np.sqrt( (2*l+1)/(2*l) )\n            root1mm = np.sqrt( (2*l-1)/(2*l-2))\n            if l == 2:\n                root1mm = np.sqrt(3)\n\n            ptmp[:,l] = rootmm/(l+1)*( l*root1mm*ptmp[:,l-2].T - (ctplus*plmplus[:,l].T -ctmin*plmmin[:,l].T)/rootmm )\n    #the arbitrary element IPlm ( computed only when lmax &gt; mfix)        \n        if lmax &gt; mfix:\n            l = mfix + 1\n        #first we do the element IPlm, for which l - m = 1    \n            rootnm = np.sqrt( (2*l+1)*(2*l-1)/(l+mfix)/(l-mfix))\n            ptmp[:,l] = rootnm/(l+1)*(np.power(stplus,2)*plmplus[:,l-1].T - np.power(stmin,2)*plmmin[:,l-1].T)\n        #now we do the rest\n            for l in range(mfix+2,lmax+1,1):              #loop over the degree l\n                rootnm = np.sqrt( (2*l+1) * (2*l-1) / (l+mfix) / (l-mfix) )\n                root1nm = np.sqrt( (2*l-1) * (2*l-3) / (l-1+mfix) / (l-1-mfix) )\n                ptmp[:,l] = rootnm/(l+1)*( (l-2)*ptmp[:,l-2].T/root1nm + np.power(stplus,2)*plmplus[:,l-1].T -np.power(stmin,2)*plmmin[:,l-1].T)\n\n# --------------------------------------------------------------------\n        # The integrated functions have been computed. What remains to be done, is to\n        # extract the proper columns from ptmp, corresponding to the vector lvec. \n        # If l or theta is scalar the output matrix p reduces to a vector. It should\n        # have the shape of respectively theta or l in that case.\n# --------------------------------------------------------------------\n\n# p     = zeros(n, length(lvec))\n    lind = np.argwhere(lvec&lt;mfix)[:,0]      #index into l &lt; m\n    pcol = lvec + 1                         #index into columns of ptmp\n    pcol[lind] = (lmax + 2)*np.ones([len(lind),1])   #Now l &lt; m points to last col\n    p = ptmp[:,pcol[:,0]-1]                 #proper column extraction \n\n    if max(lvec.shape) == 1 and min(np.array([theRAD]).shape) == 1 and trow == 1:\n        p = p.T\n    if max(np.array([theRAD]).shape) == 1 and min(lvec.shape) == 1 and lcol == 1:\n        p = p.T\n    return p\n</code></pre>"},{"location":"auxillary_codes/#grace-data-pre-processing","title":"GRACE Data Pre-Processing","text":""},{"location":"auxillary_codes/#pyshbundle.GRACEpy.lovenr","title":"<code>lovenr(lmax)</code>","text":"<p>LOVENR gives the LOVE number of the elastic earth for a certain degree n</p>"},{"location":"auxillary_codes/#pyshbundle.GRACEpy.lovenr--parameters","title":"Parameters","text":"<p>lmax : int     Spherical harmonic degree (up to 200)</p>"},{"location":"auxillary_codes/#pyshbundle.GRACEpy.lovenr--returns","title":"Returns:","text":"<p>kn : int     LOVE number of degree lmax</p> <p>REMARKS: The elastic LOVE numbers are taken from the paper by WAHR et al.,  \"Time variability of the earth's gravity field: hydrological and  oceanic effects and their possible detection using GRACE\", JGR, Vol. 103, No. B12, p 30205-30229, 1998</p> <p>Created on Mon May 11 11:09:28 2022</p> <p>author: Dr. Bramha Dutt Vishwakarma, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/GRACEpy.py</code> <pre><code>def lovenr(lmax: int):\n    \"\"\"LOVENR gives the LOVE number of the elastic earth for a certain degree n\n\n    Parameters\n    ----------\n    lmax : int\n        Spherical harmonic degree (up to 200)\n\n    Returns:\n    ----------\n    kn : int\n        LOVE number of degree lmax\n\n    REMARKS:\n    The elastic LOVE numbers are taken from the paper by WAHR et al., \n    \"Time variability of the earth's gravity field: hydrological and \n    oceanic effects and their possible detection using GRACE\",  \n    JGR, Vol. 103, No. B12, p 30205-30229, 1998\n\n    Created on Mon May 11 11:09:28 2022\n\n    _author_: Dr. Bramha Dutt Vishwakarma, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n    l  = [0,  1,    2,    3,    4,    5,   6,   7,   8,   9,  10,  12,  15,  20,  30,  40,  50,  70, 100, 150, 200]\n    kl = numpy.divide([0,270,-3030,-1940,-1320,-1040,-890,-810,-760,-720,-690,-640,-580,-510,-400,-330,-270,-200,-140,-100, -700],1e4)\n    n = range(0, lmax+1, 1)\n    kn = numpy.interp(n,l,kl)\n    return(kn)\n</code></pre>"},{"location":"auxillary_codes/#pyshbundle.GRACEpy.lovenrPREM","title":"<code>lovenrPREM(lmax, frame)</code>","text":"<p>lovenrprem calculates the LOVE and Shida number of the elastic earth for a certain degree n in different reference frame values in given degrees for LOVE numbers as provided by Olivier Francis  from the PREM Earth model for selected degrees</p>"},{"location":"auxillary_codes/#pyshbundle.GRACEpy.lovenrPREM--parameters","title":"Parameters","text":"<p>lmax : int     Spherical harmonic degree (up to 200)</p>"},{"location":"auxillary_codes/#pyshbundle.GRACEpy.lovenrPREM--returns","title":"Returns","text":"<p>kn,hn,ln: int     load LOVE number of degree n</p> REMARKS <p>see also lovenr</p> <p>Created on Mon May 11 11:51:29 2022</p> <p>@author:  Dr. Bramha Dutt Vishwakarma, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/GRACEpy.py</code> <pre><code>def lovenrPREM(lmax:int, frame):\n    \"\"\" \n\n    lovenrprem calculates the LOVE and Shida number of the elastic earth for a\n    certain degree n in different reference frame\n    values in given degrees for LOVE numbers as provided by Olivier Francis \n    from the PREM Earth model for selected degrees\n\n    Parameters\n    ----------\n    lmax : int\n        Spherical harmonic degree (up to 200)\n\n    Returns\n    ----------\n    kn,hn,ln: int\n        load LOVE number of degree n\n\n    REMARKS:\n        see also lovenr\n    Created on Mon May 11 11:51:29 2022\n\n    @author:  Dr. Bramha Dutt Vishwakarma, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n\n    data = numpy.array([[ 1,  -0.28476,   0.00000,   0.10462],\n    [2,  -0.99297,  -0.61274,   0.04661] ,\n    [3,  -1.05142,  -0.58897 ,  0.21048] ,\n    [4,  -1.05378,  -0.53513 ,  0.23564] ,\n    [5,  -1.08658,  -0.52382 ,  0.23186] ,\n    [6,  -1.14404,  -0.54222 ,  0.23263] ,\n    [7,  -1.21254,  -0.57464 ,  0.24058] ,\n    [8,  -1.28403,  -0.61256 ,  0.25308] ,\n    [9,  -1.35479,  -0.65203 ,  0.26799] ,\n    [10,  -1.42330,  -0.69140,   0.28419] ,\n    [11,  -1.48909,  -0.72998,   0.30121] ,\n    [12,  -1.55204,  -0.76749,   0.31880] ,\n    [13,  -1.61221,  -0.80381,   0.33684] ,\n    [14,  -1.66968,  -0.83886,   0.35522] ,\n    [15,  -1.72454,  -0.87260,   0.37382] ,\n    [16,  -1.77684,  -0.90499,   0.39251] ,\n    [17,  -1.82668,  -0.93599,   0.41119] ,\n    [18,  -1.87414,  -0.96560,   0.42973] ,    \n    [19,  -1.91928,  -0.99382,   0.44804] ,\n    [20,  -1.96220,  -1.02066,   0.46603] ,\n    [21,  -2.00297,  -1.04614,   0.48363] ,\n    [22,  -2.04169,  -1.07029,   0.50078] ,\n    [23,  -2.07844,  -1.09313,   0.51742] ,\n    [24,  -2.11332,  -1.11472,   0.53355] ,\n    [25,  -2.14642,  -1.13511,   0.54912] ,\n    [30,  -2.28839,  -1.22067,   0.61848] ,\n    [40,  -2.48641,  -1.33024,   0.71925] ,\n    [50,  -2.61710,  -1.39016,   0.78410] ,\n    [60,  -2.71254,  -1.42377,   0.82683] ,\n    [70,  -2.78865,  -1.44313,   0.85550] ,\n    [80,  -2.85368,  -1.45474,   0.87479] ,\n    [90,  -2.91216,  -1.46226,   0.88764] ,\n    [100,  -2.96672,  -1.46787,   0.89598] ,\n    [120,  -3.06983,  -1.47811,   0.90421] ,\n    [140,  -3.16950,  -1.49082,   0.90634] ,\n    [160,  -3.26809,  -1.50771,   0.90603] ,\n    [180,  -3.36633,  -1.52909,   0.90532] ,\n    [200,  -3.48436,  -1.55473,   0.90547] ,\n    [250,  -3.70773,  -1.63448,   0.91388] ,\n    [300,  -3.94607,  -1.73053,   0.93714] ,\n    [350,  -4.17591,  -1.83593,   0.97495] ,\n    [400,  -4.39433,  -1.94515,   1.02467] ,\n    [500,  -4.78872,  -2.15940,   1.14615] ,\n    [600,  -5.12008,  -2.35243,   1.27714] ,\n    [800,  -5.59959,  -2.64798,   1.50995] ,\n    [1000,  -5.88447,  -2.83157,  1.67325] ,\n    [1500,  -6.15106,  -3.00957,   1.84797] ,\n    [2000,  -6.20058,  -3.04408,   1.88423] ,\n    [3000,  -6.21044,  -3.05176,   1.89114] ,\n    [5000,  -6.21155,  -3.05324,   1.89118] ,\n    [10000,  -6.21226,  -3.05427,   1.89110]])\n\n    l  =  data[:,0]\n    hl =  data[:,1]\n    kl =  numpy.divide(data[:,2], l)\n    ll =  numpy.divide(data[:,3], l)\n\n    if frame == 'CM':\n        hl[0] = hl[0] - 1\n        ll[0] = ll[0] - 1\n        kl[0] = kl[0] - 1\n        print('Love numbers are in center of mass frame')\n    elif frame == 'CF':\n        hlo = hl[0]\n        llo = ll[0]\n        hl[0] = (hlo - llo) * 2/3\n        ll[0] = (hlo - llo) * (-1/3)\n        kl[0] = ((-2/3)*llo) - ((-1/3)*hlo)\n        print('Love numbers are in center of figure frame')\n    elif frame == 'CE':\n        print('Love numbers are in center of solid Earth frame')\n    else:\n        lovenrPREM.exit('Please choose a compatible frame of reference: one of CM, CF, or CE')\n\n\n    n = range(0, lmax+1, 1)\n    kn = numpy.interp(n,l,kl)\n    hn = numpy.interp(n,l,hl)\n    ln = numpy.interp(n,l,ll)\n    kn[0] = 0 \n    hn[0] = 0\n    ln[0] = 0\n    return(kn,hn,ln)\n</code></pre>"},{"location":"auxillary_codes/#pyshbundle.GRACEpy.upwcon","title":"<code>upwcon(degree, height)</code>","text":"<p>Returns the upward continuation \\((R/r)^l\\)</p>"},{"location":"auxillary_codes/#pyshbundle.GRACEpy.upwcon--parameters","title":"Parameters","text":"<p>degree :int     Spherical harmonic degree height : int     Height above mean Earth radius m</p>"},{"location":"auxillary_codes/#pyshbundle.GRACEpy.upwcon--returns","title":"Returns","text":"<pre><code>uc : array\n    Upward continuation terms\n</code></pre> Uses <p><code>GRACEconstants.GC</code></p> REMARKS <p>If both degree and height are vectors, degree will be(come) a row vector and height a column vector. load necessary constants</p> Todo <ul> <li>Add input checking functionality and raise exceptions</li> <li>Add reference to formula</li> </ul>"},{"location":"auxillary_codes/#pyshbundle.GRACEpy.upwcon--created-on-sat-may-9-184945-2022","title":"Created on Sat May  9 18:49:45 2022","text":"Source code in <code>pyshbundle/GRACEpy.py</code> <pre><code>def upwcon(degree: int, height):\n    \"\"\"Returns the upward continuation $(R/r)^l$\n\n    Parameters\n    ----------\n    degree :int\n        Spherical harmonic degree\n    height : int\n        Height above mean Earth radius [m] [scalar/vector]\n\n    Returns\n    -------\n        uc : array\n            Upward continuation terms\n\n    Uses:\n        `GRACEconstants.GC`\n\n    REMARKS:\n        If both degree and height are vectors, degree will be(come) a row vector\n        and height a column vector.\n        load necessary constants\n\n    Todo:\n        + Add input checking functionality and raise exceptions\n        + Add reference to formula\n    # Created on Sat May  9 18:49:45 2022\n    \"\"\"\n    rr = numpy.divide(GC.ae, numpy.add(GC.ae,height))\n    uc = numpy.power(rr, degree)\n\n    return(uc)    \n</code></pre>"},{"location":"auxillary_codes/#filtering-the-grace-data","title":"Filtering the GRACE Data","text":"<p>Generates values for a Gaussian smoothing filter</p> <p>The program delivers the spherical harmonic coefficients of a gaussian smoothing filter. The coefficients are calculated according to Wahr et. al.(1998) equation (34) and Swenson and Wahr equation (34)</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.Gaussian--parameters","title":"Parameters","text":"<p>L : int     Maximum degree of the spherical harmonics cap : int     half width of Gaussian smoothing function [km]</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.Gaussian--returns","title":"Returns","text":"<p>W: numpy.ndarray     smoothing coefficients</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Degree must be integer</p> <code>ValueError</code> <p>Maximum degree must be higher than 2</p> <code>TypeError</code> <p>Cap size must be an integer</p> References <p>Wahr et.al. (1998) equation (34) and Swenson and Wahr equation (34)</p> Author <p>Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/shutils.py</code> <pre><code>def Gaussian(L: int, cap: int):\n    \"\"\"Generates values for a Gaussian smoothing filter\n\n    The program delivers the spherical harmonic coefficients of a gaussian\n    smoothing filter. The coefficients are calculated according to Wahr et. al.(1998)\n    equation (34) and Swenson and Wahr equation (34)\n\n\n    Parameters\n    ----------\n    L : int\n        Maximum degree of the spherical harmonics\n    cap : int\n        half width of Gaussian smoothing function [km]\n\n    Returns\n    -------\n    W: numpy.ndarray\n        smoothing coefficients\n\n    Raises:\n        TypeError: Degree must be integer\n        ValueError: Maximum degree must be higher than 2\n        TypeError: Cap size must be an integer\n\n    References:\n        Wahr et.al. (1998) equation (34) and Swenson and Wahr equation (34)\n\n    Author:\n        Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n\n    #Check input\n    if type(L) != int:\n        raise TypeError('Degree must be integer')\n\n    if L&lt;2:\n        raise ValueError('Maximum degree must be higher than 2')\n\n    if type(cap) != int:\n        raise TypeError('Cap size must be an integer')\n\n    #Calculations\n    W = np.zeros([L+1, 1])\n    b = np.log(2)/(1 - np.cos(cap/6371))\n\n    #Recursive calculation of the weighting coefficients\n    W[0,0] = 1\n    W[1,0] = np.power(10, np.log10( (1 + np.exp(-2*b))/(1-np.exp(-2*b)) - (1/b)))\n\n    i = 1\n    while i &lt; L:        \n        j = i + 1\n        W[i+1][0] = W[i-1][0] - (2*(j-1) + 1)/b * W[i][0]\n        if W[i+1, 0] &gt; W[i] or W[i+1] &lt; 0:\n            W[i+1] = 0\n        i = i + 1\n\n    return W\n</code></pre>"},{"location":"auxillary_codes/#numerical-integration","title":"Numerical Integration","text":"<p>This function computes Gauss base points and weight factors using the algorithm-see Reference</p> <p>This function uses cubic interpolation to replace NaNs</p> <p>Returns the weights and nodes for Neumann's numerical integration</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.grule--parameters","title":"Parameters","text":"<p>n : int      number of base points required</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.grule--returns","title":"Returns","text":"<p>bp : numpy.array      cosine of the base points wf : numpy.array     weight factors for computing integrals and such</p> References <ol> <li>'Methods of Numerical Integration' by Davis and Rabinowitz, page 365, Academic Press, 1975.</li> </ol> Author <p>Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/shutils.py</code> <pre><code>def grule(n: int):\n    \"\"\"This function computes Gauss base points and weight factors\n    using the algorithm-see Reference\n\n    Parameters\n    ----------\n    n : int \n        number of base points required\n\n    Returns\n    ----------\n    bp : numpy.array \n        cosine of the base points\n    wf : numpy.array\n        weight factors for computing integrals and such\n\n    References:\n        1. 'Methods of Numerical Integration' by Davis and Rabinowitz, page 365, Academic Press, 1975.\n\n    Author:\n        Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n    bp = np.zeros((n,1))\n    wf = bp\n    iter = 2\n    m = np.floor((n+1)/2)\n    e1 = n * (n+1)\n\n\n    mm = 4*m - 1\n    t = (np.pi / (4*n + 2)) * np.arange(3,mm+4,4)\n    nn = (1 - (1 - 1/n)/(8*n*n))\n    x0 = nn * np.cos(t)\n\n\n    for i in range(iter):\n        pkm1 = 1\n        pk = x0\n\n        for kk in range(n-1):\n            k = kk + 2\n            t1 = x0 * pk\n            pkp1 = t1 - pkm1 - (t1-pkm1)/k  + t1\n            pkm1=pk\n            pk=pkp1\n\n        den = 1 - x0*x0\n        d1 = n * (pkm1 - x0*pk)\n        dpn = d1/den\n\n\n        d2pn = (2*x0*dpn - e1*pk) / den\n        d3pn = (4*x0*d2pn + (2-e1)*dpn)/den\n        d4pn = (6*x0*d3pn + (6-e1)*d2pn)/den\n        u = pk/dpn\n        v = d2pn/dpn\n        h = -u * (1+(.5*u)*(v+u*(v*v - u*d3pn/(3*dpn))))\n        p = pk + h*(dpn+(0.5*h)*(d2pn+(h/3)*(d3pn + 0.25*h*d4pn)))\n        dp = dpn + h*(d2pn+(0.5*h)*(d3pn+h*d4pn/3))\n        h = h-p/dp\n        x0 = x0+h\n\n    bp = -x0-h\n    fx = d1 - h*e1*(pk+(h/2)*(dpn+(h/3)*(d2pn+(h/4)*(d3pn+(0.2*h)*d4pn))))\n    wf = [2 * (1 - np.power(bp,2))]/(fx*fx)\n\n\n    for i in range(len(bp),n):\n        bp = np.append(bp,[0])\n        wf = np.append(wf,[0])\n\n    if ((m)+(m)) != (n):\n        m = m-1\n\n    for i in range(1,int(m+1)):\n        bp[-i] = -bp[i-1]\n        wf[-i] = wf[i-1] \n    return bp, wf\n</code></pre>"},{"location":"auxillary_codes/#pyshbundle.shutils.naninterp--parameters","title":"Parameters","text":"<p>X : (numpy.array)     array with NaN values</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.naninterp--returns","title":"Returns","text":"<p>numpy.array     cubic interpolated array</p> Source code in <code>pyshbundle/shutils.py</code> <pre><code>def naninterp(X):\n    \"\"\"This function uses cubic interpolation to replace NaNs\n\n    Parameters\n    ----------\n    X : (numpy.array)\n        array with NaN values\n\n    Returns\n    ----------\n    numpy.array\n        cubic interpolated array\n    \"\"\"\n\n    ok = ~np.isnan(X)\n    xp = ok.ravel().nonzero()[0] #Indices of xs with values\n    fp = X[~np.isnan(X)]\n\n    x  = np.isnan(X).ravel().nonzero()[0] #Indices of xs without values\n\n    pchip = PchipInterpolator(xp,fp) #Initialize scipy PHCIP cubic interpolation\n    X[np.isnan(X)] = pchip(x) #Interpolate Nan values in X\n\n    return X\n</code></pre>"},{"location":"auxillary_codes/#pyshbundle.shutils.neumann--parameters","title":"Parameters","text":"<p>inn : int or numpy.array     base points (nodes) in the interval [-1;1]</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.neumann--returns","title":"Returns","text":"<p>w : array     quadrature weights x : array     base points (nodes) in the interval [-1;1]</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Integer input argument required</p> <code>ValueError</code> <p>Error in input dimensions</p> Remarks <ul> <li>1st N.-method: see Sneeuw (1994) GJI 118, pp 707-716, eq. 19.5</li> <li>2nd N.-method: see uberall/GRULE</li> </ul> Todo <ul> <li>TypeError is more relavant and shape error from np</li> </ul> Uses <p><code>grule</code>, <code>plm</code></p> Author <p>Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/shutils.py</code> <pre><code>def neumann(inn):\n    \"\"\"Returns the weights and nodes for Neumann's numerical integration\n\n    Parameters\n    ----------\n    inn : int or numpy.array\n        base points (nodes) in the interval [-1;1]\n\n    Returns\n    ----------\n    w : array\n        quadrature weights\n    x : array\n        base points (nodes) in the interval [-1;1]\n\n    Raises:\n        TypeError: Integer input argument required\n        ValueError: Error in input dimensions\n\n    Remarks:\n        * 1st N.-method: see Sneeuw (1994) GJI 118, pp 707-716, eq. 19.5\n        * 2nd N.-method: see uberall/GRULE\n\n    Todo: \n        + TypeError is more relavant and shape error from np\n\n    Uses:\n        `grule`, `plm`\n\n    Author:\n        Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n\n    try: #if input is an integer\n        x, w = grule(inn)\n    except: #if input is an array\n        if(len(inn)==1): #2nd Neumann method\n            x, w = grule(inn)\n            if(np.not_equal(np.mod(x, 1), 0)): #Not integer\n                raise TypeError(\"Integer input argument required\")\n\n\n\n        elif min(inn.shape) == 1: #1st Neumann method #Size gives 2 outputs for 2d array in matlab; for row and column\n            x = inn\n            theRAD = np.arccos(x) #x in radian\n            l = np.array(list(range(len(x))))\n            pp = plm(l, theRAD)\n\n            rr = list([2])\n            for i in len(x-1):\n                rr.append(0)\n            r = np.asarray(rr)\n\n            w,resid,rank,s = np.linalg.lstsq(pp,r) #Solve system of equations; Double check this operation\n            if(x.shape != w.shape):\n                w = w.T\n\n        else:\n            raise ValueError(\"Error in input dimensions\")\n            # TO DO: Write more descriptive exception messages\n\n    return w, x\n</code></pre>"},{"location":"auxillary_codes/#important-for-spherical-harmonic-synthesis","title":"Important for Spherical Harmonic Synthesis","text":"<p>Returns the function F from the spectra A and B</p> <p>NORMALKLM returns an ellipsoidal normal field consisting of normalized -Jn, n=0,2,4,6,8</p> <p>Returns the isotropic spectral transfer (or: eigenvalues) of several gravity related quantities.  Upward continuation may be included.</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.ispec--parameters","title":"Parameters","text":"<p>a : int      cosine coefficients b : (int, optional)     sine coefficients. Defaults to -9999.</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.ispec--returns","title":"Returns","text":"<pre><code>f (numpy.ndarray: **fill**\n</code></pre> See Also <p><code>spec</code></p> Author <p>Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/shutils.py</code> <pre><code>def ispec(a,b = -9999):\n    \"\"\"Returns the function F from the spectra A and B\n\n    Parameters\n    ----------\n    a : int \n        cosine coefficients\n    b : (int, optional)\n        sine coefficients. Defaults to -9999.\n\n    Returns\n    ----------\n        f (numpy.ndarray: **fill**\n\n    See Also:\n        `spec`\n\n    Author:\n        Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n\n    n2 = a.shape[0]\n    a[0,:] = a[0, :]*2\n\n\n    if (np.absolute(b[n2-1,:]) &lt; 1e-10).all():\n        n = 2 * n2 - 2     \n        a[n2-1,:] = a[n2-1,:] * 2            \n        fs = (a - 1j * b)/2\n        fs  = (np.concatenate((fs,np.conj(fs[np.arange(n2-2,0,-1),:])), axis = 0))*max(n,1)\n\n    else:\n        n = 2 * n2 - 1                        \n        fs = (a - 1j * b)/2\n        fs = (np.concatenate((fs,np.conj(fs[np.arange(n2-1,0,-1),:])), axis = 0))*n\n\n    f = np.real(ifft(fs.T).T)\n    return f\n</code></pre>"},{"location":"auxillary_codes/#pyshbundle.shutils.normalklm--parameters","title":"Parameters","text":"<p>lmax : int     Maximum degree of the spherical harmonics typ : (str, optional)      Ellipsoids can be either          'wgs84' - World Geodetic System 84,          'grs80' - ,          'he' - hydrostatic equilibrium ellipsoid</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.normalklm--returns","title":"Returns","text":"<p>nklm : numpy.array      normal field in CS-format (sparse array - [1, -J2, -J4, -J6, -J8])</p> TODO <p>Find type of nklm; I think raising TypeError, VlueError or NameError instad of general Exception</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>lmax should be an integer</p> <code>ValueError</code> <p>lmax should be positive</p> <code>ValueError</code> <p>Unknown type of ellipsoid, supports 'wgs84', <code>GRS80</code> and 'he'</p> References <ol> <li>J2,J4 values for hydrostatic equilibrium ellipsoid from Lambeck (1988) \"Geophysical Geodesy\", p.18 </li> </ol> Author <p>Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/shutils.py</code> <pre><code>def normalklm(lmax: int, typ: str = 'wgs84'):\n    \"\"\" NORMALKLM returns an ellipsoidal normal field\n    consisting of normalized -Jn, n=0,2,4,6,8\n\n    Parameters\n    ----------\n    lmax : int\n        Maximum degree of the spherical harmonics\n    typ : (str, optional) \n        Ellipsoids can be either \n            'wgs84' - World Geodetic System 84, \n            'grs80' - , \n            'he' - hydrostatic equilibrium ellipsoid\n\n    Returns\n    ----------\n    nklm : numpy.array\n         normal field in CS-format (sparse array - [1, -J2, -J4, -J6, -J8])\n\n    TODO: \n        Find type of nklm; I think raising TypeError, VlueError or NameError instad of general Exception\n\n    Raises:\n        TypeError: lmax should be an integer\n        ValueError: lmax should be positive\n        ValueError: Unknown type of ellipsoid, supports 'wgs84', `GRS80` and 'he'\n\n    References:\n        1. J2,J4 values for hydrostatic equilibrium ellipsoid from Lambeck (1988)\n        \"Geophysical Geodesy\", p.18 \n\n    Author:\n        Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n\n    if type(lmax) != int:\n        raise TypeError(\"lmax should be integer\")\n\n    if lmax &lt; 0:\n        raise ValueError(\"lmax should be positive\")\n\n\n    typ_ = typ.lower()\n    if (typ_ == 'wgs84'):\n        J2     =  1.08262982131e-3     #% earth's dyn. form factor (= -C20 unnormalized)\n        J4     = -2.37091120053e-6    #% -C40 unnormalized\n        J6     =  6.08346498882e-9     #% -C60 unnormalized\n        J8     = -1.42681087920e-11    #% -C80 unnormalized\n        jcoefs = np.array([1, -J2, -J4, -J6, -J8]).T.reshape(5,1)\n        # as lmax + 2 is requires \n        l      = np.arange(0,min(lmax + 2,8 + 2), 2).T\n        l.reshape(l.shape[0],1)\n\n    elif (typ_ == 'grs80'):\n        J2     =  1.08263e-3         # % earth's dyn. form factor (= -C20 unnormalized)\n        J4     = -2.37091222e-6     #% -C40 unnormalized\n        J6     =  6.08347e-9        #% -C60 unnormalized\n        J8     = -1.427e-11         #% -C80 unnormalized\n        jcoefs = np.array([1, -J2, -J4, -J6, -J8]).reshape(5,1)\n        l      = np.arange(0,min(lmax + 2,8 + 2), 2).T\n        l.reshape(l.shape[0],1)\n\n    elif ((typ_ == 'he') or (typ_ == 'hydro')):\n        J2     = 1.072618e-3\t\t#% earth's dyn. form factor (= -C20 unnormalized)\n        J4     = 0.2992e-5     \t#% -C40 unnormalized\n        jcoefs = np.array([1, -J2, -J4]).T.reshape(5,1)\n        # adding (2) beacuse of arange function is only uptp last integer and not including last\n        l      = np.arange(0,min(lmax + 2,4 + 2), 2).T\n        l.reshape(l.shape[0],1)\n\n    else:\n        raise ValueError(\"Unknown type of ellipsoid:   \", typ)\n\n    coefs = jcoefs[:len(l)].T / np.sqrt(2*l + 1)\n#    coefs.reshape(coefs.shape[0],1)\n\n\n    data = np.array(coefs)[0]\n    row = np.array(l)\n    col = np.zeros(len(l))\n    # lmax = 96 then shape=(97, 97) -&gt; consisitent with everything else\n    nklm = sparse.coo_matrix((data,(row,col)),shape=(lmax+1,lmax+1)).toarray()\n    return nklm\n</code></pre>"},{"location":"auxillary_codes/#pyshbundle.shutils.eigengrav--parameters","title":"Parameters","text":"<p>lmax : int     Maximum degree of Spherical Coefficients fstr : str     denoting the functional under consideration:     'none',      'geoid',     'dg', 'gravity' ... gravity anomaly,     'potential',      'tr' .............. gravity disturbance,      'trr' ............. (d^2/dr^2)     'slope' ........... size of surface gradient,      'water' ........... equivalent water thickness,      'smd' ............. surface mass density.     'height' .......... vertical displacements     h (float): height above Earth mean radius [m].</p>"},{"location":"auxillary_codes/#pyshbundle.shutils.eigengrav--returns","title":"Returns","text":"<p>numpy.ndarray     Transfer matrix. Size and shape equal to lmax. Units are respectively          [none], [m], [mGal], [mGal], [E], [m^2/s^2], [rad], [m], [kg/m^2].                                                        [n x 1] Uses:     upwcon, lovenr, uberall/constants, uberall/isint</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Enter a valid lmax value</p> Author <p>Dr. Bramha Dutt Vishwakarma, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/shutils.py</code> <pre><code>def eigengrav(lmax: int, fstr: str, h: float):\n    \"\"\"\n\n    Returns the isotropic spectral transfer (or: eigenvalues) of several gravity related quantities. \n    Upward continuation may be included.\n\n    Parameters\n    ----------\n    lmax : int\n        Maximum degree of Spherical Coefficients\n    fstr : str\n        denoting the functional under consideration:\n        'none', \n        'geoid',\n        'dg', 'gravity' ... gravity anomaly,\n        'potential', \n        'tr' .............. gravity disturbance, \n        'trr' ............. (d^2/dr^2)\n        'slope' ........... size of surface gradient, \n        'water' ........... equivalent water thickness, \n        'smd' ............. surface mass density.\n        'height' .......... vertical displacements\n        h (float): height above Earth mean radius [m].\n\n    Returns\n    ----------\n    numpy.ndarray\n        Transfer matrix. Size and shape equal to lmax. Units are respectively \n            [none], [m], [mGal], [mGal], [E], [m^2/s^2], [rad], [m], [kg/m^2].\n                                                           [n x 1]\n    Uses:\n        upwcon, lovenr, uberall/constants, uberall/isint\n\n    Raises:\n        TypeError: Enter a valid lmax value\n\n    Author:\n        Dr. Bramha Dutt Vishwakarma, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n\n    if type(lmax) == int:\n        rows = 1\n    else:\n        rows = len(lmax)\n    # rows = len(l)\n\n    if rows &gt; 1 or lmax &lt; 0:\n        raise TypeError(\"Enter a valid lmax value\")\n\n    r = GC.ae + h\n\n    # lmax issue - using lmax as per used in shbundle\n    # no reference for height\n\n    if fstr == 'none':\n        tf = np.ones((1, lmax+1))\n    elif fstr == 'geoid':\n        tf = np.ones((1, lmax+1)) * r\n    elif fstr == 'potential':\n        tf = np.ones((1, lmax+1)) * (GC.GM/r)\n    elif fstr == 'gravity' or fstr == 'dg':\n        tf = np.multiply(range(-1, lmax, 1), ((GC.GM/r/r) * 1e5))\n        tf = tf.reshape((1, lmax+1))\n    elif fstr == 'tr':\n        tf = np.multiply(range(-1, -(lmax+2), -1), ((GC.GM/r/r) * 1e5))\n        tf = tf.reshape((1, lmax+1))\n    elif fstr == 'trr':\n        tf = np.multiply(range(1, (lmax+2), 1),\n                            range(2, (lmax + 3), 1))*((GC.GM/r/r) * 1e9)\n        tf = tf.reshape((1, lmax+1))\n    elif fstr == 'slope':\n        tf = np.sqrt(np.multiply(\n            range(0, lmax+1, 1), range(1, lmax+2, 1)))\n        tf = tf.reshape((1, lmax+1))\n    elif fstr == 'water':\n        ln = GB.lovenr(lmax)\n        tf = np.divide(np.multiply(\n            5.517*r, np.add(range(0, 2*lmax + 1, 2), 1)), np.multiply(3, (1+ln)))\n        tf = tf.reshape((1, lmax+1))\n    elif fstr == 'smd':\n        ln = GB.lovenr(lmax)\n        tf = np.divide(np.multiply(\n            5517*r, np.add(range(0, 2*lmax + 1, 2), 1)), np.multiply(3, (1+ln)))\n        tf = tf.reshape((1, lmax+1))\n    elif fstr == 'height':\n        kl, hl, ll = GB.lovenrPREM(90, 'CF')\n        tf = np.divide(np.multiply(hl, (GC.ae*1000)), np.add(kl, 1))\n    else:\n        ValueError('Requested functional FSTR not available.')\n\n    if h &gt; 0:\n        upConTerm = GB.upwcon(lmax, h)\n        tf = np.multiply(tf, upConTerm)\n\n    return(tf)\n</code></pre>"},{"location":"auxillary_codes/#time-series","title":"Time Series","text":""},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways.</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/mn5hk/pyshbundle/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#found-bugs","title":"Found Bugs!!!","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#want-new-functionality","title":"Want New Functionality?","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#lets-improve-documentation","title":"Let's Improve Documentation","text":"<p>pyshbundle could always use more documentation, whether as part of the official pyshbundle docs, in docstrings, or even on the web in blog posts, articles, and such.</p> <p>Another prefered way is to create explainatory tutorials. Using the <code>PySHBundle</code> functions to explain the <code>Spherical Harmonics</code> and <code>GRACE Data Processing</code>.</p>"},{"location":"contributing/#feedback","title":"Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/mn5hk/pyshbundle/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up pyshbundle for local development.</p> <ol> <li> <p>Fork the pyshbundle repo on GitHub.</p> </li> <li> <p>Setup a seperate development environment.     <pre><code># clone the repo and fetch the dev branch\n$ git clone git@github.com:mn5hk/pyshbundle.git\n\n# creating a new virtual environment\n$ python3 -m venv &lt;name-env&gt;\n\n# install the dependencies from the requirements-dev file\n$ pip install -r ../pyshbundle/requirements-dev.txt\n\n# activate the virtual environment environment\n$ source &lt;/location-of-virt-env/name-env/bin/activate&gt;\n</code></pre></p> </li> <li> <p>Build the latest repo in the development virtual environment.     <pre><code># install package into virtual environment\n$ pip install ../pyshbundle/dist/&lt;required-version&gt;.tar.gz\n\n# you also have the option to build the module using, be careful of \n$ python setup.py sdist\n</code></pre></p> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes lo    cally.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <p>TBD...</p>"},{"location":"convert_data_formats/","title":"Convert Data Formats","text":"<p>Converts SH coefficients from CS to SC format</p> <p>converts the square (L+1)x(L+1) matrix 'field', containing spherical harmonics coefficients in CS storage format into a  rectangular (L+1)x(2L+1) matrix in  SC format.</p> <p>Converts the format from CLM to CS</p> <p>Under the hood uses the <code>clm2sc</code> and <code>sc2cs</code> function</p> <p>Converts the spherical harmonic coefficients from clm format to SC format</p> <p>Converts the spherical harmonic coefficients from klm format to SC format</p>"},{"location":"convert_data_formats/#pyshbundle.cs2sc--parameters","title":"Parameters","text":"<p>field : numpy.ndarray     The square (L+1)x(L+1) np matrix field , containing                spherical harmonics coefficients in CS storage format</p>"},{"location":"convert_data_formats/#pyshbundle.cs2sc--returns","title":"Returns","text":"<p>numpy.ndarray     Rectangular (L+1)x(2L+1) np matrix in  SC format</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Input neither in cs nor in sc format</p> Todo <ul> <li>Rather use TypeError instead of base Exception</li> </ul> <p>Examples:</p> <p>cs2sc(field)</p> Source code in <code>pyshbundle/reshape_SH_coefficients.py</code> <pre><code>def cs2sc(field):\n    \"\"\"Converts SH coefficients from CS to SC format\n\n    converts the square (L+1)x(L+1) matrix 'field', containing\n    spherical harmonics coefficients in CS storage format into a \n    rectangular (L+1)x(2L+1) matrix in  SC format.\n\n    Parameters\n    ----------\n    field : numpy.ndarray\n        The square (L+1)x(L+1) np matrix field , containing\n                   spherical harmonics coefficients in CS storage format\n\n    Returns\n    ----------\n    numpy.ndarray\n        Rectangular (L+1)x(2L+1) np matrix in  SC format\n\n    Raises:\n        TypeError: Input neither in cs nor in sc format\n\n    Todo:\n        + Rather use TypeError instead of base Exception\n\n    Examples:\n        cs2sc(field)\n    \"\"\"\n\n    rows = len(field)\n    cols = len(field[0])\n\n    if (rows != cols) and (cols != 2*rows - 1):\n        raise TypeError(\"Input neither in cs nor in sc format\")\n    elif cols == 2*rows - 1:\n        sc = field\n    else:\n        c    = np.tril(field)\n        ut   = np.triu(field)\n        i = np.identity(rows)\n        i = 1-i\n        s    = np.fliplr(np.transpose(np.multiply(ut, i, )))\n        sc   = np.concatenate((s[:,1:rows], c), axis=1)\n\n    return(sc)\n</code></pre>"},{"location":"convert_data_formats/#pyshbundle.sc2cs.sc2cs","title":"<code>sc2cs(field)</code>","text":"<p>Converts SH coefficients from SC to CS format</p> <p>converts the rectangular \\((L+1) * (2L+1)\\) matrix FIELD, containing spherical harmonics coefficients in SC storage format into a  square (L+1)x(L+1) matrix in CS format.</p>"},{"location":"convert_data_formats/#pyshbundle.sc2cs.sc2cs--parameters","title":"Parameters","text":"<p>field : numpy.ndarray the rectangular (L+1)x(2L+1) matrix, containing the spherical harmonics coefficients in SC storage format</p>"},{"location":"convert_data_formats/#pyshbundle.sc2cs.sc2cs--returns","title":"Returns","text":"<p>cs : numpy.ndarray     square (L+1)x(L+1) matrix in CS format</p> References <p>See the SHBundle docs or PySHBundle docs for more info about SH coeff. storage and retrival formats being implementd.</p> <p>Examples:</p> <p>sc2cs(field)</p> Source code in <code>pyshbundle/sc2cs.py</code> <pre><code>def sc2cs(field):\n    \"\"\"Converts SH coefficients from SC to CS format\n\n    converts the rectangular $(L+1) * (2L+1)$ matrix FIELD, containing\n    spherical harmonics coefficients in SC storage format into a \n    square (L+1)x(L+1) matrix in CS format.\n\n    Parameters\n    ----------\n    field : numpy.ndarray\n    the rectangular (L+1)x(2L+1) matrix, containing the\n    spherical harmonics coefficients in SC storage format\n\n    Returns\n    ----------\n    cs : numpy.ndarray\n        square (L+1)x(L+1) matrix in CS format\n\n    References:\n        See the SHBundle docs or PySHBundle docs for more info about SH coeff. storage and retrival formats being implementd.\n\n    Examples:\n        sc2cs(field)\n    \"\"\"\n\n    rows = len(field)\n    cols = len(field[0])\n\n    if (rows!=cols) and (cols!=2*rows - 1):\n        sc2cs.exit(\"Input neither in cs nor in sc format\")\n    elif cols == rows:\n        cs = field\n    else:\n        c    = field[:, rows-1:cols]\n        st   = numpy.transpose(numpy.fliplr(field[:, 0:rows-1]))\n        z    = numpy.zeros([1,rows])\n        s    = numpy.concatenate((st, z), axis=0)\n        cs   = numpy.add(c, s)\n\n    return(cs)\n</code></pre>"},{"location":"convert_data_formats/#pyshbundle.clm2cs--parameters","title":"Parameters","text":"<p>data_mat : numpy.ndarray      list containing [degree;  order; clm; slm; delta clm; delta slm; start data; end date] lmax : int     Max Degree of the spherical harmonic expansion sigma_flag : (bool, optional)     Flag to return the standard deviation data in CS format or not. Defaults to False</p>"},{"location":"convert_data_formats/#pyshbundle.clm2cs--returns","title":"Returns","text":"<p>numpy.array     Spherical Harmonic Coefficients in CS format</p> Source code in <code>pyshbundle/reshape_SH_coefficients.py</code> <pre><code>def clm2cs(data_mat: np.ndarray, lmax: int, sigma_flag=False):\n    \"\"\"Converts the format from CLM to CS\n\n    Under the hood uses the `clm2sc` and `sc2cs` function\n\n    Parameters\n    ----------\n    data_mat : numpy.ndarray \n        list containing [degree;  order; clm; slm; delta clm; delta slm; start data; end date]\n    lmax : int\n        Max Degree of the spherical harmonic expansion\n    sigma_flag : (bool, optional)\n        Flag to return the standard deviation data in CS format or not. Defaults to False\n\n    Returns\n    ----------\n    numpy.array\n        Spherical Harmonic Coefficients in CS format\n\n    \"\"\"\n    if sigma_flag:\n        sc_mat, dev_sc = clm2sc(data_mat=data_mat, lmax=lmax, sigma_flag=True)\n        return sc2cs(sc_mat), sc2cs.sc2cs(dev_sc)\n    else:\n        sc_mat = clm2sc(data_mat=data_mat, lmax=lmax, sigma_flag=False)\n        return sc2cs(sc_mat)\n</code></pre>"},{"location":"convert_data_formats/#pyshbundle.clm2sc--parameters","title":"Parameters","text":"<p>data_mat : numpy.ndarray      list containing [degree;  order; clm; slm; delta clm; delta slm; start data; end date] lmax : int     Max Degree of the spherical harmonic expansion sigma_flag : (bool, optional)     Flag to return the standard deviation data in CS format or not. Defaults to False</p>"},{"location":"convert_data_formats/#pyshbundle.clm2sc--returns","title":"Returns","text":"<p>numpy.array     Spherical Harmonic Coefficients in SC format</p> References <p>Refer to the SHBundle or PySHBundle docs for the different data storage and retrival formats.</p> Source code in <code>pyshbundle/reshape_SH_coefficients.py</code> <pre><code>def clm2sc(data_mat: np.ndarray, lmax: int, sigma_flag=False):\n    \"\"\"Converts the spherical harmonic coefficients from clm format to SC format\n\n    Parameters\n    ----------\n    data_mat : numpy.ndarray \n        list containing [degree;  order; clm; slm; delta clm; delta slm; start data; end date]\n    lmax : int\n        Max Degree of the spherical harmonic expansion\n    sigma_flag : (bool, optional)\n        Flag to return the standard deviation data in CS format or not. Defaults to False\n\n    Returns\n    ----------\n    numpy.array\n        Spherical Harmonic Coefficients in SC format\n\n    References:\n        Refer to the SHBundle or PySHBundle docs for the different data storage and retrival formats.\n\n    \"\"\"    \n\n    sc_mat = np.zeros((lmax+1, 2*lmax + 2))\n    dev_sc_mat = np.zeros((lmax+1, 2*lmax + 2))\n\n    # as per the convention\n    clm = data_mat[:, 2]\n    slm = data_mat[:, 3]\n    clm_std_dev = data_mat[:, 4]\n    slm_std_dev = data_mat[:, 5]\n\n    i = 0\n    for index1 in range(0,lmax+1, 1):\n        for index2 in range(0,index1+1, 1):\n\n            sc_mat[index1, lmax-index2] = slm[i]\n            sc_mat[index1, lmax+index2+1] = clm[i]\n\n            dev_sc_mat[index1, lmax-index2] = slm_std_dev[i]\n            dev_sc_mat[index1, lmax+index2+1] = clm_std_dev[i]\n\n            i = i + 1\n\n    sc_mat = np.delete(sc_mat, lmax, 1)\n    dev_sc_mat = np.delete(dev_sc_mat, lmax, 1)\n\n    if sigma_flag:\n        return sc_mat, dev_sc_mat\n    else:\n        return sc_mat\n</code></pre>"},{"location":"convert_data_formats/#pyshbundle.klm2sc--parameters","title":"Parameters","text":"<p>data_mat : numpy.ndarray      list containing [degree;  order; clm; slm; delta clm; delta slm; start data; end date] lmax : int     Max Degree of the spherical harmonic expansion sigma_flag : (bool, optional)     Flag to return the standard deviation data in CS format or not. Defaults to False</p>"},{"location":"convert_data_formats/#pyshbundle.klm2sc--returns","title":"Returns","text":"<p>numpy.array     Spherical Harmonic Coefficients or/and associated standard deviations in SC format</p> Source code in <code>pyshbundle/reshape_SH_coefficients.py</code> <pre><code>def klm2sc(data_mat: np.ndarray, lmax: int, sigma_flag=False):\n    \"\"\"Converts the spherical harmonic coefficients from klm format to SC format\n\n    Parameters\n    ----------\n    data_mat : numpy.ndarray \n        list containing [degree;  order; clm; slm; delta clm; delta slm; start data; end date]\n    lmax : int\n        Max Degree of the spherical harmonic expansion\n    sigma_flag : (bool, optional)\n        Flag to return the standard deviation data in CS format or not. Defaults to False\n\n    Returns\n    ----------\n    numpy.array\n        Spherical Harmonic Coefficients or/and associated standard deviations in SC format\n    \"\"\"\n    sc_mat = np.zeros((lmax+1, 2*lmax + 2))\n    dev_sc_mat = np.zeros((lmax+1, 2*lmax + 2))\n    clm = data_mat[:, 2]\n    slm = data_mat[:, 3]\n    clm_std_dev = data_mat[:, 4]\n    slm_std_dev = data_mat[:, 5]\n\n    # first place the slm and then clm\n    index2 =0\n    for index1 in range(0,lmax+1,1):\n        sc_mat[index1:, lmax-index1] = slm[(index2):(index2 + lmax-index1+1)]\n        sc_mat[index1:, index1+lmax] = clm[(index2):(index2 + lmax-index1+1)]\n\n        dev_sc_mat[index1:, lmax-index1] = slm_std_dev[(index2):(index2 + lmax-index1+1)]\n        dev_sc_mat[index1:, index1+lmax] = clm_std_dev[(index2):(index2 + lmax-index1+1)]\n\n        index2 = index2 + lmax-index1+1\n\n    sc_mat=np.delete(sc_mat,lmax,axis=1)\n    dev_sc_mat=np.delete(dev_sc_mat,lmax,axis=1)\n\n    if sigma_flag:\n        return sc_mat, dev_sc_mat\n    else: \n        return sc_mat\n</code></pre>"},{"location":"convert_data_formats/#reference","title":"Reference","text":"<ul> <li>Nico Sneeuw, Matthias Weigelt, Markus Antoni, Matthias Roth, Balaji Devaraju, et. al. (2021). SHBUNDLE 2021. http://www.gis.uni-stuttgart.de/research/projects/Bundles.</li> </ul>"},{"location":"core_functionality/","title":"Core functionality","text":"<p>This module has both Spherical Harmonics Analysis and Spherical Harmonics Synthesis</p>"},{"location":"core_functionality/#spherical-harmonic-analysis-and-synthesis","title":"Spherical Harmonic Analysis and Synthesis","text":"<p>GSHA - Global Spherical Harmonic Analysis, inverse of GSHS.</p> <p>gshs - Global Spherical Harmonic Synthesis</p> <p>calculates the phase difference between two time series based on the Hilbert transform method explained by Phillip et al.</p>"},{"location":"core_functionality/#pyshbundle.pysh_core.gsha--parameters","title":"Parameters","text":"<p>f : numpy.ndarray     Global field of size \\((l_{max} + 1) * 2 * l_{max}\\) or \\(l_{max} * 2 * l_{max}\\) method : str     Method to be used. grid : (str, optional)     choose between 'block' or 'cell'. Defaults to None. lmax : (int, optional)     Maximum degree of development. Defaults to -9999.</p>"},{"location":"core_functionality/#pyshbundle.pysh_core.gsha--returns","title":"Returns","text":"<p>np.ndarray     Spherical harmonics coefficients Clm, Slm in |C\\S| format</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>grid argument can only be 'block' or 'cell'</p> <code>ValueError</code> <p>Grid type entered is not right</p> <code>TypeError</code> <p>Invalid size of matrix F</p> <code>TypeError</code> <p>GRID and METHOD must be strings</p> <code>ValueError</code> <p>2nd Neumann method ONLY on a ''neumann''/''gauss'' GRID'</p> <code>ValueError</code> <p>Block mean method ONLY on a ''block''/''cell'' GRID</p> <code>ValueError</code> <p>Maximum degree of development is higher than number of rows of input.</p> <p>REMARKS: TBD - Zlm-functions option     - eigengrav, GRS80     - When 'pole' grid, m = 1 yields singular Plm-matrix!</p> Uses <p><code>plm</code>, <code>neumann</code>, <code>iplm</code>, <code>sc2cs</code></p> Author <p>Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/pysh_core.py</code> <pre><code>def gsha(f, method: str, grid: str = None, lmax: int = -9999):\n    \"\"\" GSHA - Global Spherical Harmonic Analysis, inverse of GSHS.\n\n    Parameters\n    ----------\n    f : numpy.ndarray\n        Global field of size $(l_{max} + 1) * 2 * l_{max}$ or $l_{max} * 2 * l_{max}$\n    method : str\n        Method to be used.\n    grid : (str, optional)\n        choose between 'block' or 'cell'. Defaults to None.\n    lmax : (int, optional)\n        Maximum degree of development. Defaults to -9999.\n\n    Returns\n    ----------\n    np.ndarray\n        Spherical harmonics coefficients Clm, Slm in |C\\S| format\n\n    Raises:\n        ValueError: grid argument can only be 'block' or 'cell'\n        ValueError: Grid type entered is not right\n        TypeError: Invalid size of matrix F\n        TypeError: GRID and METHOD must be strings\n        ValueError: 2nd Neumann method ONLY on a ''neumann''/''gauss'' GRID'\n        ValueError: Block mean method ONLY on a ''block''/''cell'' GRID\n        ValueError: Maximum degree of development is higher than number of rows of input.\n\n    REMARKS:\n    TBD - Zlm-functions option\n        - eigengrav, GRS80\n        - When 'pole' grid, m = 1 yields singular Plm-matrix!\n\n    Uses:\n        `plm`, `neumann`, `iplm`, `sc2cs`\n\n    Author:\n        Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n    rows, cols = f.shape\n\n    if cols == 2 * rows: #Check conditions\n        if lmax == -9999:\n            lmax = rows\n\n        if grid == None:\n            grid = 'block'\n\n        if (grid != 'block') and (grid != 'cell'):\n            raise ValueError(\"Your GRID variable should be either block or cell\")\n\n        n = rows\n        dt = 180 / n\n        theta = np.arange(dt/2, 180+(dt/4), dt)\n        lam = np.arange(dt/2, 360+(dt/4), dt)\n\n    elif cols == 2 * rows - 2:\n        if lmax == -9999:\n            lmax = rows - 1\n        if grid == None:\n            grid = 'pole'\n\n        n = rows - 1\n        dt = 180 / n\n\n        if (grid == 'pole') or (grid == 'mesh'):                   \n            theta = np.arange(0, 180+(dt/4), dt)\n            lam = np.arange(0, 360+(dt/4) - dt, dt)\n        elif (grid == 'neumann') or (grid == 'gauss'): \n        # gw, gx = neumann(n+1) #For some reason, grule does not work for even values\n            gw, gx = neumann(n)\n            theta = np.arccos(np.flipud(gx)) * 180 / np.pi\n            lam = np.arange(0, 360+(dt/4)-dt, dt)\n\n            if len(gw.shape) == 1:\n                gw = gw.reshape(gw.shape[0],1)\n\n            if len(gx.shape) == 1:\n                gx = gx.reshape(gx.shape[0],1)\n        else:\n            raise ValueError(\"Grid type entered is incorrect\")\n    else:\n        raise TypeError(\"Invalid size of matrix F\")\n\n    theRAD = theta * np.pi / 180\n    # if len(theRAD.shape) == 1:\n    # theRAD = theRAD.reshape(theRAD.shape[0],1)\n\n\n    # further diagnostics\n\n    if (type(grid) != str) or (type(method) != str):\n        raise TypeError(\"GRID and METHOD must be strings.\")\n\n    if (method == 'snm') and ((grid != 'neumann') and (grid != 'gauss')):\n        raise ValueError('2nd Neumann method ONLY on a ''neumann''/''gauss'' GRID')\n\n    if (method == 'mean') and ((grid != 'block') and (grid != 'cell')):\n        raise ValueError('Block mean method ONLY on a ''block''/''cell'' GRID')\n\n    if lmax &gt; n:\n        raise ValueError('Maximum degree of development is higher than number of rows of input.')\n\n    # Reshape variables as required\n\n    if len(lam.shape) == 1:\n        lam = lam.reshape(1,lam.shape[0])\n\n    # Init\n\n    L = n\n    clm = np.zeros((L+1, L+1), dtype='float')\n    slm = np.zeros((L+1, L+1), dtype='float')\n\n\n    # First step of analysis\n\n    m = np.arange(L+1).reshape(1,L+1)\n    c = np.cos((lam.T @ m) * np.pi/180)\n    s = np.sin((lam.T @ m) * np.pi/180)\n\n\n    # % preserving the orthogonality (except for 'mean' case)\n    # % we distinguish between 'block' and 'pole' type grids (in lambda)\n\n    if (grid == 'block') or (grid == 'cell'):\n        if method == 'mean':\n            dl = dt\n            c[:,0] = dl / 360\n            m = np.arange(1, L+1)\n            ms = 2 / m * np.sin(m * dl/2 * np.pi/180) / np.pi\n            c[:,1:(L+1)+1] = c[:,1:(L+1)+1] * ms  \n            s[:,1:(L+1)+1] = s[:,1:(L+1)+1] * ms\n\n        else:\n            c = c/L\n            s = s/L\n            c[:,0] = c[:,1]/2\n            s[:,L] = s[:,L]/2\n            c[:,L] = np.zeros(2*n)\n            s[:,0] = np.zeros(2*n)\n    else:\n        c = c/L\n        s = s/L\n        c[:,[0, L]] = c[:,[0, L]]/2\t\n        s[:,[0, L]] = np.zeros((2*n,2))\t  \n\n\n    a = f @ c\n    b = f @ s    \n\n    # Second step of analysis: Clm and Slm\n\n    if method == 'ls':\n        for m in range(L+1):\n#            l = np.arange(m,L+1)\n            l = np.arange(m,L+1).reshape(L+1-m, 1)\n            l = l.T\n\n            p = plm(l,m,theRAD, 3, 1)\n            p = p[:,:,0]\n            ai = a[:, m]\n            bi = b[:, m]\n\n            clm[m+1:L+2, m+1] = linalg.lstsq(p, ai)\n            slm[m+1:L+2, m+1] = linalg.lstsq(p, bi)\n\n\n    elif method == 'aq': #Approximate Quadrature\n        si = np.sin(theRAD)\n        si = 2 * si / np.sum(si)\n\n        for m in range(L+1):\n            l = np.arange(m, L+1).reshape(L+1-m, 1)\n            l = l.T\n\n            p = plm(l,m,theRAD, 3, 1)\n\n            ai = a[:, m]\n            bi = b[:, m]\n\n            clm[m:L+1, m] = (1 + (m == 0))/ 4 * p.T @ (si * ai)\n            slm[m:L+1, m] = (1 + (m == 0))/ 4 * p.T @ (si * bi)\n\n    elif method == 'fnm': #1st Neumann method (exact upto L/2)\n        w = neumann(np.cos(theRAD))\n\n        for m in range(L+1):\n            l = np.arange(m, L+1).reshape(L+1-m, 1)\n            l = l.T\n\n            p = plm(l,m,theRAD, 3, 1)\n\n            ai = a[:, m]\n            bi = b[:, m]\n\n            clm[m:L+1, m] = (1 + (m == 0))/ 4 * p.T @ (w * ai)\n            slm[m:L+1, m] = (1 + (m == 0))/ 4 * p.T @ (w * bi)\n\n    elif method == 'snm': #2nd Neumann method (exact)\n        for m in range(L+1):\n            l = np.arange(m, L+1).reshape(L+1-m, 1)\n            l = l.T\n\n            p = plm(l,m,theRAD, 3, 1)\n\n            ai = a[:, m]\n            bi = b[:, m]\n\n            clm[m:L+1, m] = (1 + (m == 0))/ 4 * p.T @ (gw * ai)\n            slm[m:L+1, m] = (1 + (m == 0))/ 4 * p.T @ (gw * bi)\n\n    elif method == 'mean':\n        for m in range(L+1):\n            print(m)\n            #l = np.arange(m,L+1).reshape(L+1-m,1)\n            #l = l.T\n\n\n            l = np.array([np.arange(m,L+1, 1)])\n        # l = np.array([[m]])\n\n            p = iplm(l,m,theRAD)\n        # p = p[:,-1]\n            ai = a[:, m]\n            bi = b[:, m]\n\n            clm[m:L+1, m] = (1 + (m == 0))/ 4 * p.T @ ai\n            slm[m:L+1, m] = (1 + (m == 0))/ 4 * p.T @ bi\n\n    # Write the coefficients Clm &amp; Slm in |C\\S| format\n\n    slm = np.fliplr(slm)\n    cs = sc2cs(np.concatenate((slm[:, np.arange(L)], clm), axis = 1))\n    cs = cs[:int(lmax+1), :int(lmax+1)]\n\n\n    return cs\n</code></pre>"},{"location":"core_functionality/#pyshbundle.pysh_core.gshs--parameters","title":"Parameters","text":"<p>field : numpy.ndarray     Matrix of SH coefficients, either in SC-triangle or CS-square format quant : (str, optional)     defining the field quantity. Defaults to 'none'. grd : (str, optional)     defining the grid. Defaults to 'mesh'. n : (int, optional)      Defaults to -9999. h : (int, optional)     Defaults to 0. jflag : (int, optional)     Defaults to 1.</p>"},{"location":"core_functionality/#pyshbundle.pysh_core.gshs--returns","title":"Returns","text":"<p>f : numpy.ndarray)     the global Spherical Harmonics feild field theRAD numpy.array     vector of co-latitudes in radians lamRAD : numpy.array     vector of longitudes in radians</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Check format of the field</p> <code>Exception</code> <p>n must be scalar</p> <code>Exception</code> <p>n must be integer</p> <code>Exception</code> <p>Grid argument must be string</p> <code>Exception</code> <p>description</p> Uses <p><code>cs2sc</code>, <code>normalklm</code>, <code>plm</code>, <code>eigengrav</code>, <code>ispec</code></p> Todo <ul> <li>Change general exceptions to specific and descriptive built-in ones</li> <li>using the not and then check is not always advisable</li> <li>Check how to document valid options</li> </ul> Author <p>Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc) Vivek Kumar Yadav, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/pysh_core.py</code> <pre><code>def gshs(field, quant = 'none', grd = 'mesh', n = -9999, h = 0, jflag = 1):\n    \"\"\"gshs - Global Spherical Harmonic Synthesis\n\n    Parameters\n    ----------\n    field : numpy.ndarray\n        Matrix of SH coefficients, either in SC-triangle or CS-square format\n    quant : (str, optional)\n        defining the field quantity. Defaults to 'none'.\n    grd : (str, optional)\n        defining the grid. Defaults to 'mesh'.\n    n : (int, optional) \n        Defaults to -9999.\n    h : (int, optional)\n        Defaults to 0.\n    jflag : (int, optional)\n        Defaults to 1.\n\n    Returns\n    ----------\n    f : numpy.ndarray)\n        the global Spherical Harmonics feild field\n    theRAD numpy.array\n        vector of co-latitudes in radians\n    lamRAD : numpy.array\n        vector of longitudes in radians\n\n    Raises:\n        Exception: Check format of the field\n        Exception: n must be scalar\n        Exception: n must be integer\n        Exception: Grid argument must be string\n        Exception: _description_\n\n    Uses:\n        `cs2sc`, `normalklm`, `plm`, `eigengrav`, `ispec`\n\n    Todo: \n        * Change general exceptions to specific and descriptive built-in ones\n        + using the not and then check is not always advisable\n        + Check how to document valid options\n\n    Author:\n        Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n        Vivek Kumar Yadav, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n\n    wd = getcwd()\n    chdir(wd)\n\n    rows, cols = field.shape\n\n    if rows == cols:                    #field in CS-format \n        lmax = rows - 1\n        field = cs2sc(field)\n    elif cols - 2 * rows == -1:         #field in SC-format already\n        lmax = rows - 1\n    else:\n        raise Exception(\"Check format of the field\")\n\n    if n == -9999:                      #(no value of n is input) -&gt; set n = lmax\n        n = lmax\n\n    if not np.isscalar(n):\n        raise Exception(\"n must be scalar\")\n\n    if not np.issubdtype(type(n), np.integer):\n        raise Exception(\"n must be integer\")\n\n    if not type(grd) == str:\n        raise Exception(\"Grid argument must be string\")\n\n    grd = grd.lower()\n\n\n\n    #Grid Definition\n    dt = np.pi/n\n\n    if grd == 'pole' or grd == 'mesh':\n        theRAD = np.arange(0, np.pi+dt*0.5, dt, dtype='float')\n        lamRAD = np.arange(0, 2*np.pi, dt, dtype='float')\n    elif grd == 'block' or grd == 'cell':\n        theRAD = np.arange(dt/2, np.pi + dt*0.5, dt, dtype='float')\n        lamRAD = np.arange(dt/2, 2*np.pi + dt*0.5, dt, dtype='float')\n    else:\n        raise Exception(\"Incorrect grid type input\")\n\n    nlat = len(theRAD)\n    nlon = len(lamRAD)\n\n\n\n#   -------------------------------------------------------------------------\n#   Preprocessing on the coefficients:\n        # - subtract reference field (if jflag is set)\n        # - specific transfer\n        # - upward continuation\n#   -------------------------------------------------------------------------\n\n    if jflag:\n        field = field - cs2sc(normalklm(lmax+1))\n\n    l = np.arange(0, lmax+1)\n    transf = np.array([eigengrav(lmax, quant, h)])[0, :, :].T\n\n    field = field * np.matmul(transf, np.ones((1, 2*lmax+1)), dtype='float')\n\n\n\n# -------------------------------------------------------------------------\n        # Size declarations and start the waitbar:\n        # Note that the definition of dlam causes straight zero-padding in case N &gt; L.\n        # When N &lt; L, there will be zero-padding up to the smallest integer multiple\n        # of N larger than L. After the Fourier transformation (see below), the\n        # proper samples have to be picked out, with stepsize dlam.\n# -------------------------------------------------------------------------\n\n\n    dlam = int(np.ceil(lmax/n))             #longitude step size\n    abcols = dlam*n + 1                     #columns required in A and B\n    a = np.zeros((nlat, int(abcols)), dtype='float')\n    b = np.zeros((nlat, int(abcols)), dtype='float')\n\n\n\n    m = 0\n    c = field[m:lmax+1, lmax+m] \n    l = np.array([np.arange(m,lmax+1)])\n    p = plm(l, m, theRAD, nargin = 3, nargout = 1)[:,:,0]\n    a[:, m] = np.dot(p,c) \n    b[:, m] = np.zeros(nlat) \n\n\n\n    for m in range(1,lmax+1,1):\n        c = field[m:lmax+1,lmax+m]\n        s = field[m:lmax+1,lmax-m]\n\n        l = np.array([np.arange(m,lmax+1)])\n        p = plm(l, m, theRAD, nargin = 3, nargout = 1)[:,:,0]\n        a[:, m] = np.dot(p,c)\n        b[:, m] = np.dot(p,s)\n\n    del field\n\n\n#   -------------------------------------------------------------------------\n        # The second synthesis step consists of an inverse Fourier transformation\n        # over the rows of a and b. \n        # In case of 'block', the spectrum has to be shifted first.\n        # When no zero-padding has been applied, the last b-coefficients must be set to\n        # zero. Otherwise the number of longitude samples will be 2N+1 instead of 2N.\n        # For N=L this corresponds to setting SLL=0!\n#  -------------------------------------------------------------------------\n\n\n    if grd =='block' or grd == 'cell': \n      m      = np.arange(0,abcols,1)\n      cshift = np.array([np.ones(nlat)], dtype='float').T * np.array([np.cos(m*np.pi/2/n)], dtype='float');\t# cshift/sshift describe the \n      sshift = np.array([np.ones(nlat)], dtype='float').T * np.array([np.sin(m*np.pi/2/n)], dtype='float');\t# half-blocksize lambda shift.\n      atemp  =  cshift*a + sshift*b\n      b      = -sshift*a + cshift*b\n      a      = atemp\n\n\n\n    if np.remainder(n,lmax) == 0:               #Case without zero-padding\n        b[:,abcols-1] = np.zeros(nlat)\n\n    #Code for ispec\n\n    f = ispec(a.T, b.T).T\n    if dlam &gt; 1: \n        f = f[:,np.arange(1,dlam*nlon+1,dlam)]\n\n    return f, theRAD, lamRAD\n</code></pre>"},{"location":"core_functionality/#pyshbundle.pysh_core.PhaseCalc--parameters","title":"Parameters","text":"<p>fts : numpy.array     time-series 1 ffts : numpy.narray     time-series 2</p>"},{"location":"core_functionality/#pyshbundle.pysh_core.PhaseCalc--returns","title":"Returns","text":"<p>numpy.ndarray     Phase difference between the two time series</p> References <ol> <li>Phillips, T., R. S. Nerem, B. Fox-Kemper, J. S. Famiglietti, and B. Rajagopalan (2012), The influence of ENSO on global terrestrial water storage using GRACE, Geophysical Research Letters, 39 (16), L16,705, doi:10.1029/2012GL052495.</li> </ol> Author <p>Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/pysh_core.py</code> <pre><code>def PhaseCalc(fts, ffts):\n    \"\"\"calculates the phase difference between two time series based on the\n    Hilbert transform method explained by Phillip et al.\n\n    Parameters\n    ----------\n    fts : numpy.array\n        time-series 1\n    ffts : numpy.narray\n        time-series 2\n\n    Returns\n    ----------\n    numpy.ndarray\n        Phase difference between the two time series\n\n    References:\n        1. Phillips, T., R. S. Nerem, B. Fox-Kemper, J. S. Famiglietti, and B. Rajagopalan (2012),\n        The influence of ENSO on global terrestrial water storage using GRACE, Geophysical\n        Research Letters, 39 (16), L16,705, doi:10.1029/2012GL052495.\n\n    Author:\n        Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n    c = fts.shape[1]\n\n    ps = np.zeros((1, c))\n\n    filter_ = ~np.isnan(fts)\n    filter__ = ~np.isnan(ffts)\n\n    fts_ = fts[filter_] #Extract values and leave Nan\n    ffts_ = ffts[filter__] #Extract values and leave Nan\n\n    fts = fts_.reshape(int(fts_.shape[0]/c),c)\n    ffts = ffts_.reshape(int(ffts_.shape[0]/c),c)\n\n    rn = fts.shape[0]\n\n    for i in range(c):\n        # A = np.concatenate(np.ones((rn,1)), np.real(signal.hilbert(ffts[:, i])), np.imag(signal.hilbert(ffts[:, i]))) #design matrix\n\n        A = np.array((np.ones((rn)), np.real(signal.hilbert(ffts[:, i])), np.imag(signal.hilbert(ffts[:, i])))).T\n\n        A = A.astype('double')\n        B = fts[:,i]\n        B = B.astype('double')\n        abc = np.linalg.lstsq(A.T @ A, A.T @ B)[0]\n\n        ps[0,i] = np.arctan2(abc[3-1],abc[2-1])*(180/np.pi) #check indices and degree/radian\n    return ps\n</code></pre>"},{"location":"core_functionality/#intro-to-grace-data-driven-correction","title":"Intro to Grace Data Driven Correction","text":"<p>Signal leakage correction using data-driven methods</p> <p>When GRACE data is applied for hydrological studies, the signal leakage is a common problem. This function uses data-driven methods to correct signal leakage in GRACE data. Please refer to paper (1) above for more details</p>"},{"location":"core_functionality/#pyshbundle.pysh_core.GRACE_Data_Driven_Correction_Vishwakarma--parameters","title":"Parameters","text":"<p>F : numpy.ndarray     A cell matrix with one column containing SH coefficients cf : int     the column in F that contains SH coefficients from GRACE GaussianR : float     Radius of the Gaussian filter (recommened = 400) basins : numpy.ndarray     mask functions of basin, a cell data format with one     column and each entry is a 360 x 720 matrix with 1 inside the     catchment and 0 outside</p>"},{"location":"core_functionality/#pyshbundle.pysh_core.GRACE_Data_Driven_Correction_Vishwakarma--returns","title":"Returns","text":"<p>(every output has a size (number of months x basins)) RecoveredTWS : numpy.ndarray     Corrected data-driven time-series (Least Squares fit method) RecoveredTWS2 : numpy.ndarray     Corrected data-driven time-series (shift and amplify method) FilteredTS : numpy.ndarray     Gaussian filtered GRACE TWS time-series for all the basins. </p> <p>Raises:</p> Type Description <code>Exception</code> <p>corrected data-driven time-series (Least Squares fit method)</p> <code>Exception</code> <p>corrected data-driven time-series (shift and amplify method)</p> <code>Exception</code> <p>gaussian filtered GRACE TWS time-series for all the basins.</p> Author <p>Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/pysh_core.py</code> <pre><code>def GRACE_Data_Driven_Correction_Vishwakarma(F, cf, GaussianR, basins):\n    \"\"\"Signal leakage correction using data-driven methods\n\n    When GRACE data is applied for hydrological studies, the signal leakage is a common\n    problem. This function uses data-driven methods to correct signal leakage in GRACE data.\n    Please refer to paper (1) above for more details\n\n\n    Parameters\n    ----------\n    F : numpy.ndarray\n        A cell matrix with one column containing SH coefficients\n    cf : int\n        the column in F that contains SH coefficients from GRACE\n    GaussianR : float\n        Radius of the Gaussian filter (recommened = 400)\n    basins : numpy.ndarray\n        mask functions of basin, a cell data format with one\n        column and each entry is a 360 x 720 matrix with 1 inside the\n        catchment and 0 outside\n\n    Returns\n    ----------\n    (every output has a size (number of months x basins))\n    RecoveredTWS : numpy.ndarray\n        Corrected data-driven time-series (Least Squares fit method)\n    RecoveredTWS2 : numpy.ndarray\n        Corrected data-driven time-series (shift and amplify method)\n    FilteredTS : numpy.ndarray\n        Gaussian filtered GRACE TWS time-series for all the basins. \n\n    Raises:\n        Exception: corrected data-driven time-series (Least Squares fit method)\n        Exception: corrected data-driven time-series (shift and amplify method)\n        Exception: gaussian filtered GRACE TWS time-series for all the basins.\n\n\n    Author:\n        Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n    deg = 0.5\n    x = np.linspace(0, 360-deg, int(360/deg))\n    y = np.linspace(0, 180-deg, int(180/deg))\n    x1 = np.linspace(deg, 360, int(360/deg))\n    y1 = np.linspace(deg, 180, int(180/deg))\n    lambdd,theta = np.meshgrid(x,y)  \n    lambdd1,theta1 = np.meshgrid(x1,y1)\n\n    theta_rad = deg_to_rad(theta)\n    theta1_rad = deg_to_rad(theta1)\n\n    #Areahalfdeg = (6378.137**2)*np.power(10,6)*np.pi/180*(np.multiply(a,b)) #Area matrix\n    Areahalfdeg = (6378.137**2)*(((np.pi/180)*lambdd1) - ((np.pi/180)*lambdd))*(np.sin((np.pi/2) - theta_rad) - np.sin((np.pi/2) - theta1_rad))\n\n    qty = 'water'\n\n    if type(F) != np.ndarray:\n        raise Exception(\"input GRACE field should be in Numpy Ndarray format, please check guidelines\")\n\n\n    if type(basins) != np.ndarray:\n        raise Exception(\"input basin field should be in Numpy NdArray format, please check guidelines\")\n\n\n    r = F.shape[0] #No of entries in F numpy ndarrray\n\n    cid = 1 #number of river catchments\n\n    f = F[:,cf-1:cf]\n    l = f[0][0].shape[0]\n    cfield = f[0][0].shape[1]\n    if cfield == l:\n        flag_cs = 0\n    else:\n        flag_cs = 1\n\n    Weights = Gaussian(l-1, GaussianR) \n    #gaussian returns weights as a list #gaussian is np.array()\n\n    try: #Broadcase Weights into dimensions\n        filter_ = np.ones([1,(2*(l-1))+1]) * Weights\n    except:\n        w0 = Weights.shape[0]\n        Weights = Weights.reshape(w0,1)\n        filter_ = np.ones([1,(2*(l-1))+1]) * Weights\n\n\n    #SH Synthesis\n    if l == cfield:\n        for m in range(r):\n            if flag_cs == 0:\n                Ft = cs2sc(f[m][0]).astype('double') \n            else:\n                Ft = f[m][0].astype('double') \n\n\n            fFld__, _, _ = gshs(Ft * filter_, qty, 'cell', int(180/deg), 0, 0) \n            ffFld__, _, _ = gshs((Ft * filter_ * filter_), qty, 'cell', int(180/deg), 0, 0)\n\n            if m == 0:\n                fFld = np.zeros((r,fFld__.shape[0],fFld__.shape[1]), dtype='double') \n                ffFld = np.zeros((r, ffFld__.shape[0], ffFld__.shape[1]), dtype='double')\n\n            fFld[m] = fFld__\n            ffFld[m] = ffFld__\n\n        long = 360/deg\n        Area = Areahalfdeg\n    else:\n        raise Exception(\"enter CS coefficients\")\n\n\n\n    #Declaration of size of the vectors:\n    cid = len(basins) #Here basins is a dictionary with each element storing nd array\n    tsleaktotalf = np.zeros([r, cid], dtype='double')\n    tsleaktotalff = np.zeros([r, cid], dtype='double')\n\n    ftsleaktotal = np.zeros([r, cid], dtype='double')\n    fftsleaktotal = np.zeros([r, cid], dtype='double')\n\n    lhat = np.zeros([r, cid], dtype='double')\n\n    bfDevRegAv = np.zeros([r, cid], dtype='double')\n    bbfDevRegAv = np.zeros([r, cid], dtype='double')\n\n    FilteredTS = np.zeros([r, cid], dtype='double')\n    filfilts = np.zeros([r, cid], dtype='double')\n\n    leakage = np.zeros([r, cid], dtype='double')\n    leakager = np.zeros([r, cid], dtype='double')   \n\n\n\n    for rbasin in range(0, cid):\n        #Get the basin functions ready\n\n        #Basin functions, filtered basin function and transfer function Kappa\n        Rb = basins[rbasin][0] \n        csRb = gsha(Rb, 'mean', 'block', long/2) \n        csF = cs2sc(csRb[0:l, 0:l]) \n        filRb_ = gshs(csF * filter_, 'none', 'cell', int(long/2), 0, 0) \n        filRb = filRb_[0]\n        kappa = (1-Rb) * filRb\n\n\n\n        fF = np.zeros((fFld__.shape[0],fFld__.shape[1]), dtype='double')\n        ffF = np.zeros((fFld__.shape[0],fFld__.shape[1]), dtype='double')\n        for m in range(0,r):\n\n\n            fF = np.concatenate((fFld[m,:,int(fF.shape[1]/2):], fFld[m,:,:int(fF.shape[1]/2)]), axis=1)\n            ffF = np.concatenate((ffFld[m,:,int(ffF.shape[1]/2):], ffFld[m,:,:int(ffF.shape[1]/2)]), axis=1)\n            #if False:    \n            if np.isnan(fF[:20,:20]).any(): #if there is a gap in time series, fill it with NaNs\n\n\n                tsleaktotalf[m][rbasin] = np.nan\n                tsleaktotalff[m][rbasin] = np.nan\n                FilteredTS[m][rbasin] = np.nan\n                filfilts[m][0:rbasin] = np.nan\n                bfDevRegAv[m][rbasin] = np.nan\n                bbfDevRegAv[m][0:rbasin] = np.nan\n\n            else:\n                #leakage time series from filtered and twice filtered fields\n                tsleaktotalf[m][rbasin] = np.sum(fF * kappa * Area) / np.sum(Rb * Area)\n                tsleaktotalff[m][rbasin] = np.sum(ffF * kappa * Area) / np.sum(Rb * Area)\n\n                #time series from filtered fields\n                FilteredTS[m][rbasin] = np.sum(fF * Rb * Area) / np.sum(Rb * Area)\n                filfilts[m][rbasin] = np.sum(ffF * Rb * Area) / np.sum(Rb * Area)\n\n                #Deviation integral timeseries\n                bfDevRegAv[m][rbasin] = np.sum((fF * Rb - FilteredTS[m][rbasin]) * filRb * Area) / np.sum(Rb * Area) #working 2022-10-20\n                bbfDevRegAv[m][rbasin] = np.sum((ffF * Rb - filfilts[m][rbasin]) * filRb * Area) / np.sum(Rb * Area)\n                print(m)\n\n\n\n\n\n    b = list()\n    bl = list()\n    for i in range(0, cid):\n\n        A = np.ones([60,2])\n        A[:,1] = naninterp(bbfDevRegAv[:, i]) #Pchip interpolate should contain atleast two elements\n\n        lssol_ = sc.linalg.lstsq(A, naninterp(bfDevRegAv[:, i])) #returns a tuple of solution \"x\", residue and rank of matrix A; for A x = B\n        lssol = lssol_[0] \n\n        b.append(lssol[2-1])\n\n\n        A = np.ones([60,2])\n        A[:,1] = naninterp(tsleaktotalff[:, i])\n        lssol_ = sc.linalg.lstsq(A, naninterp(tsleaktotalf[:, i])) #returns a tuple of solution \"x\", residue and rank of matrix A; for A x = B\n        lssol = lssol_[0]\n        bl.append(lssol[2-1])\n        #Working till here 2022-10-21 1530pm\n\n    multp = npm.repmat(b, r, 1) \n    devint = bfDevRegAv * multp\n    multp = npm.repmat(bl, r, 1)\n    leakLS = tsleaktotalf * multp\n\n\n    ps = PhaseCalc(tsleaktotalf,tsleaktotalff)\n\n\n\n    #Compute the near true leakage\n\n    for i in range(0, cid):   \n        ftsleaktotal[:,i] = naninterp(tsleaktotalf[:,i]) #Replaces gaps (NaN values) with an itnerpolated value in the leakage time series from once filtered fields\n        fftsleaktotal[:,i] = naninterp(tsleaktotalff[:,i]) #replace the gaps (NaN values) with an interpolated value in leakage time series from twice filtered fields\n\n        X = sc.fft.fft(ftsleaktotal[:,i]) #take fast Fourier transform #check shape of X 2022-10-21\n        p = -ps[0,i] / r #compute the fraction of the time period by which the time series is to be shiftes\n        Y = np.exp(1j * np.pi * p * ((np.arange(r)) - r/2) / r) #compute the Conjugate-Symmetric shift \n        Z = X * Y #Apply the shift\n\n        a = sc.fft.ifft(Z) #apply inverse fft\n\n        con = np.conj(a)\n\n        s = a + con\n\n        z = s/2\n\n        leakage[:,i] = z #shifted timeseries\n\n\n        #Shift timeseriecs from once filtered fields in the direction of the time series from twice filtered fields, to later compute the amplitude ratio\n        p = ps[0,i] / r #Fraction of a time period to shift data\n        Y = np.exp(1j * np.pi * p * ((np.arange(r)) - r/2) / r) #compute the Conjugate-Symmetric shift\n        Z = X * Y\n\n        a = sc.fft.ifft(Z) #apply inverse fft\n\n        con = np.conj(a)\n\n        s = a + con\n\n        z = s/2\n\n        leakager[:,i] = z #shifted timeseries\n\n\n\n    #compute the ratio between the amplitude of the shifted leakage from once filtered fields and leakage from twice filtered fields\n    rfn = leakage/fftsleaktotal\n    rfn[(rfn) &gt;= 2] = 1\n    rfn[(rfn) &lt;= -2] = -1\n    rfn = np.sum(np.abs(rfn), axis = 0)\n    rfn=rfn/r # amplitude ratio\n\n\n    lhat = leakager * rfn #apply the amplitude ratio to the shifted leakage timeseries from the once filtered fields to get the near true leakage\n    lhat[np.isnan(FilteredTS)] = np.nan #reintroduce nan for data gaps\n    leakLS[np.isnan(FilteredTS)] = np.nan\n    RecoveredTWS = FilteredTS - leakLS - devint\n    RecoveredTWS2 = FilteredTS - lhat - devint\n\n    return RecoveredTWS, RecoveredTWS2, FilteredTS\n</code></pre>"},{"location":"core_functionality/#hydrological-applications-with-grace","title":"Hydrological Applications with GRACE","text":"<p>Calculate the basin average of the total water storage (TWS) from the gridded TWS data.</p> <p>Applies area weighting to the gridded TWS data and then clips the data to the basin shapefile. Followed by summation of data over the latitude and longitude dimensions, divides it by basin- area to get the basin average TWS.</p> <p>Spherical Harmonics Synthesis for Total Water Storage (TWS) calculation.</p> <p>Calculate the total water storage (TWS) from spherical harmonics coefficients. Uses spherical harmonics synthesis to go from harmonics to gridded domain.</p>"},{"location":"core_functionality/#pyshbundle.hydro.Basinaverage--parameters","title":"Parameters","text":"<p>temp : xarray.DataArray      Gridded total water storage data gs : float     grid size shp_basin : geopandas.GeoDataFrame     Shapefile of the basin basin_area : float     Area of the basin in square meters</p>"},{"location":"core_functionality/#pyshbundle.hydro.Basinaverage--returns","title":"Returns","text":"<p>xarray.DataArray     Total water storage data clipped to the basin xarray.DataArray     Basin average total water storage</p> Source code in <code>pyshbundle/hydro.py</code> <pre><code>def Basinaverage(temp, gs, shp_basin, basin_area):\n    \"\"\"Calculate the basin average of the total water storage (TWS) from the gridded TWS data.\n\n    Applies area weighting to the gridded TWS data and then clips the data to the basin shapefile.\n    Followed by summation of data over the latitude and longitude dimensions, divides it by basin-\n    area to get the basin average TWS.\n\n    Parameters\n    ----------\n    temp : xarray.DataArray \n        Gridded total water storage data\n    gs : float\n        grid size\n    shp_basin : geopandas.GeoDataFrame\n        Shapefile of the basin\n    basin_area : float\n        Area of the basin in square meters\n\n    Returns\n    ----------\n    xarray.DataArray\n        Total water storage data clipped to the basin\n    xarray.DataArray\n        Basin average total water storage\n    \"\"\"\n\n    from pyshbundle.hydro import area_weighting\n    # area_weighting returns the area of each grid in m^2 for the grid resolution specified\n    temp_weighted=temp.copy()\n    temp_weighted['tws']=temp['tws']*area_weighting(gs)\n\n    ''' add projection system to nc '''\n    basin_tws = temp_weighted.rio.write_crs(\"EPSG:4326\", inplace=True)\n    basin_tws = basin_tws.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n\n    # mask data with shapefile\n    basin_tws = basin_tws.rio.clip(shp_basin.geometry.apply(mapping), shp_basin.crs,drop=True)\n    basin_avg_tws=basin_tws.sum(dim=('lon','lat'), skipna=True)/basin_area  #basin average tws\n\n    basin_tws=basin_tws.drop_vars('spatial_ref')\n    basin_avg_tws=basin_avg_tws.drop_vars('spatial_ref')\n\n    return basin_tws, basin_avg_tws\n</code></pre>"},{"location":"core_functionality/#pyshbundle.hydro.TWSCalc--parameters","title":"Parameters","text":"<p>data : numpy.ndarray     Spherical harmonics coefficients in SC format lmax : int     Maximum degree of the spherical harmonics coefficients gs : float      grid size r : float     half-width of the Gaussian filter m : int     number of time steps</p>"},{"location":"core_functionality/#pyshbundle.hydro.TWSCalc--returns","title":"Returns","text":"<p>numpy.ndarray     Gridded TWS data</p> Uses <p>'Gaussian', 'gshs',</p> <p>Author:     Vivek Yadav, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)</p> Source code in <code>pyshbundle/hydro.py</code> <pre><code>def TWSCalc(data, lmax: int, gs: float, r:float, m: int):\n    \"\"\"Spherical Harmonics Synthesis for Total Water Storage (TWS) calculation.\n\n    Calculate the total water storage (TWS) from spherical harmonics coefficients.\n    Uses spherical harmonics synthesis to go from harmonics to gridded domain.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        Spherical harmonics coefficients in SC format\n    lmax : int\n        Maximum degree of the spherical harmonics coefficients\n    gs : float \n        grid size\n    r : float\n        half-width of the Gaussian filter\n    m : int\n        number of time steps\n\n    Returns\n    ----------\n    numpy.ndarray\n        Gridded TWS data\n\n    Uses:    \n        'Gaussian', 'gshs',\n    Author:\n        Vivek Yadav, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"\n    SC = data\n\n    gfilter = Gaussian(lmax,r)\n    grid_y = int(180/gs)\n    grid_x = int(360/gs)\n    tws_f = np.zeros([m,grid_y,grid_x], dtype ='float')\n    for i in tqdm(range(0,m,1)):\n        field = SC[i,0:lmax+1,96-lmax:96+lmax+1]\n        shfil = np.zeros([lmax+1,2*lmax+1])\n\n        for j in range(0,2*lmax+1,1):\n            shfil[:,j] = gfilter[:,0] * field[:,j]\n\n        quant = 'water' \n        grd = 'cell'\n        n = int(180/gs) \n        h = 0 \n        jflag = 0\n\n        ff = gshs(shfil, quant, grd, n, h, jflag)[0]\n\n        ff = ff*1000    # convert units from m to mm\n        tws_f[i,:,0:int(grid_x/2)] = ff[:,int(grid_x/2):]\n        tws_f[i,:,int(grid_x/2):] = ff[:,0:int(grid_x/2)]   \n\n    plt.imshow(tws_f[0,:,:])\n    return(tws_f)\n</code></pre>"},{"location":"faq/","title":"FAQ","text":"<p>Work in Progress...</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install pyshbundle, run this command in your terminal:</p> <p>This is the preferred method to install pyshbundle, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p> <pre><code># creating a new virtual environment\n$ python3 -m venv &lt;name-env&gt;\n# activate the virtual environment environment\n$ source &lt;/location-of-virt-env/name-env/bin/activate&gt;\n# install package into virtual environment\n$ pip install pyshbundle\n\n# clone the repository in order to access the notebooks and data\n$ git clone git@github.com:mn5hk/pyshbundle.git\n</code></pre>"},{"location":"installation/#from-the-source-for-devscontributors","title":"From the source - for Devs/Contributors","text":"<p>Developers can access the latest development branch and </p> <pre><code># clone the repo and fetch the dev branch\n$ git clone git@github.com:mn5hk/pyshbundle.git\n\n# creating a new virtual environment\n$ python3 -m venv &lt;name-env&gt;\n\n# install the dependencies from the requirements-dev file\n$ pip install -r ../pyshbundle/requirements-dev.txt\n\n# activate the virtual environment environment\n$ source &lt;/location-of-virt-env/name-env/bin/activate&gt;\n\n# install package into virtual environment\n$ pip install ../pyshbundle/dist/&lt;required-version&gt;.tar.gz\n\n# you also have the option to build the module using, be careful of \n$ python setup.py sdist\n</code></pre>"},{"location":"license/","title":"License:","text":"<pre><code>This file is part of PySHbundle.\nPySHbundle is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.\n</code></pre>"},{"location":"load_data/","title":"Load Data","text":""},{"location":"load_data/#_1","title":"Load Data","text":"<p>Reads the spherical harmonic data provided by JPL</p> Source code in <code>pyshbundle/io.py</code> <pre><code>def parse_jpl_header(header_info):\n\n    # parse the header info passed by the reader in as list of bytes\n    # create a dictionary with key = important params from header file\n\n    header = {}\n\n    # important info from header file\n    # Dimension - Degree and Order\n    # normalization info\n    # permanent_tide_flag\n    # earth_gravity_param - G*M_earth with units and value\n    # mean_equator_radius - units and value\n    # processing_level\n    # product_version\n    # conventions\n    # institution\n    # title\n    # time_coverage_start\n    # time_coverage_end\n    # unused_days\n\n    normal_keys = ['title', 'institution', 'product_version',\n                     'processing_level', 'normalization', 'permanent_tide_flag',\n                    ]\n    dimension_keys = ['degree', 'order']\n    date_time_keys = ['time_coverage_start', 'time_coverage_end', 'unused_days']\n    physical_constant_keys = ['earth_gravity_param', 'mean_equator_radius']\n\n    for key in normal_keys:\n        key_index_in_header = find_word(header_info, key)\n        # print(f\"{key} - header line = {key_index_in_header +1} value= {' '.join(parse_lines(header_info[key_index_in_header])[3:])[: -3]}\")\n        header[key] = ' '.join(parse_lines(header_info[key_index_in_header])[3:])[: -3]\n\n    for key in dimension_keys:\n        key_index_in_header = find_word(header_info, key)\n        val = int(\" \".join(parse_lines(header_info[key_index_in_header], parse_fmt='\\s+')[3:])[: -3])\n        # print(f\"{key} - {val}\")\n        header[key] = val\n\n    for key in date_time_keys:\n        # TODO: Look back and find what you meant.... \n        key_index_in_header = find_word(header_info, key)\n        # find a way to make it date time object so it can be used later\n        pass\n\n    for key in physical_constant_keys:\n        key_index_in_header = find_word(header_info, key)\n\n        const_long_name = ' '.join(parse_lines(header_info[key_index_in_header + 1])[3:])[: -3]\n        const_units = ' '.join(parse_lines(header_info[key_index_in_header + 2])[3:])[: -3]\n        const_value = float(' '.join(parse_lines(header_info[key_index_in_header + 3])[3:])[: -3])\n        const_dict = {'units': const_units, 'value': const_value}\n        # returning a dict with value and corresponding units\n        header[key] = const_dict\n\n    return header\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def parse_lines(line, parse_fmt='\\s+'):\n    #  parses the liness and reutrns an array\n    # '\\s+' returns array with no whitespace\n\n    parsed_array = re.split(parse_fmt, str(line))\n\n    return parsed_array\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def parse_csr_file(file_path: str):\n    # ensure that the file path is valid then proceed\n\n    source = 'CSR'\n\n    # check if the file is ziped or not\n    if file_path[-3:] == '.gz':\n        # open the file and read the lines\n        with gzip.open(file_path, 'r') as file:\n\n            # read the file line wise -&gt; obtain a list of bytes\n            info_lines = file.readlines()\n            num_lines = len(info_lines)\n\n            for i in range(len(info_lines)):\n                # find the index of line which indicates end of header info\n                if str(info_lines[i]) == str(b'# End of YAML header\\n',):\n                    end_of_header_idx = i\n                    break\n\n        header_info = info_lines[:end_of_header_idx]\n\n        # parse the header strings to extract relavant metadata info\n        csr_header = header_info # parse_jpl_header(header_info)\n\n        # parse the data strings to extract numeric data in suitable matrix fmt\n        csr_data = extract_SH_data(file_path, source='csr')\n\n        # Organize the data into either matrix, dataframe or dictionary format      \n\n    return csr_header, csr_data\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def parse_csr_header():\n\n    # similar to JPL one\n\n    raise NotImplementedError(\"Similar to `parse_jpl_header`... not yet implemented seperately.\")\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def parse_itsg_file(file_path):\n\n    # ensure that the file path is valid then proceed\n\n    source = 'CSR'\n\n    # check if the file is ziped or not\n\n    # open the file and read the lines\n    if file_path[-4:] == '.gfc':\n        with open(file_path, 'r') as file:\n\n            # read the file line wise -&gt; obtain a list of bytes\n            info_lines = file.readlines()\n            num_lines = len(info_lines)\n\n            for i in range(len(info_lines)):\n                if str(info_lines[i]) == str('end_of_head ==================================================================================\\n',):\n                    end_of_header_idx = i\n                    break\n\n        istg_header = info_lines[:end_of_header_idx]\n\n        # parse the header strings to extract relavant metadata info\n        itsg_data = extract_SH_data(file_path, source='itsg')\n\n    return istg_header, itsg_data\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def parse_itsg_header(header_info: list):\n\n    normal_keys = ['modelname', 'product_type',\n                     'norm', 'tide_system', 'errors', 'earth_gravity_constant', 'radius',\n                     'max_degree'\n                    ]\n\n    # physical_constant_keys = ['earth_gravity_constant', 'radius', ]\n\n    for key in normal_keys:\n\n        key_index_in_header = find_word(header_info, key)\n        #print(f\"{key} - header line = {key_index_in_header} value= {parse_lines(header_info[key_index_in_header])[1]}\")\n\n    model_name_idx = find_word(header_info, 'modelname')\n    date_str = parse_lines(header_info[model_name_idx])[1][-7:]\n\n    header_dict = {}\n    '''\n    for key in physical_constant_keys:\n        key_index_in_header = find_word(header_info, key)\n\n        const_long_name = parse_lines(header_info[key_index_in_header])[0]\n        const_value = float(parse_lines(header_info[key_index_in_header])[1])\n        const_dict = {'long_name': const_long_name, 'value': const_value}\n        print(const_dict)\n\n    '''\n    return header_dict, date_str\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def parse_tn13_header(header_info):\n\n    # IMP Info\n        # - Title\n        # - Last reported data point\n    # Special Notes\n        # - 1, 2, 3, 4, 5\n\n    # finding the index of important sub-headers like Title and Notes\n    for i in range(len(header_info)):\n        if 'TITLE' in header_info[i]:\n            title_idx = i\n            break\n\n    for i in range(len(header_info)):\n        if 'SPECIAL NOTES' in header_info[i]:\n            notes_idx = i\n            break\n\n    # The tile is\n    title = ' '.join(re.split(\"\\s+\", header_info[title_idx +1])[1:-1]) + ' '.join (re.split(\"\\s+\", header_info[title_idx+2])[1:-1]) + ' '.join(re.split(\"\\s+\", header_info[title_idx+3])[1:-1])\n\n    # TODO: later convert the str object to a date-time object\n    last_reported_date = (re.split(\"\\s+\", header_info[title_idx+3])[-2])[:-1]\n\n    special_notes = []\n\n    # add parsing for special notes later\n\n    return title, last_reported_date\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def parse_tn14_header():\n\n    # Key info\n        # - Title\n        # - Version\n        # - Date Span\n        # - Notes:\n\n\n    # Constants\n        # - Mean C20\n        # - Mean C30\n        # - GM\n        # R\n\n\n\n    pass\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def find_date_in_replacemnt_file(replacemnt_mat, file_type: str, epoch_begin, epoch_end=None):\n\n    # epoch_begin and epoch_end -&gt; date from the grace data file\n    # begin_date and end_data -&gt; date from the replacement file (tn-13 or tn-14)\n\n    rows, cols = replacemnt_mat.shape\n\n    if file_type == 'tn-13':\n        time_buffer_itsg = timedelta(days=23)\n        date_idxs = set()\n        # think of a rather efficient searching scheme\n        for i in range(rows):\n            begin_date = datetime.strptime(str(int(replacemnt_mat[i][-2])), '%Y%m%d').date()\n            end_date = datetime.strptime(str(int(replacemnt_mat[i][-1])), '%Y%m%d').date()\n\n            if epoch_end:\n                # for jpl and csr\n                if begin_date == epoch_begin and end_date == epoch_end:\n                    date_idxs.add(i)\n                    print(f\"epoch-begin: {epoch_begin}, epoch-end: {epoch_end}, start: {begin_date}, end: {end_date}\")\n            else:\n                # for itsg\n                #begin_date = f\"{begin_date.year}-{str(begin_date.month).zfill(2)}\"\n                if type(epoch_begin) == str:\n                    epoch_begin = datetime.strptime(epoch_begin, \"%Y-%m\").date()\n\n                if begin_date - time_buffer_itsg &lt;= epoch_begin &lt;= begin_date + time_buffer_itsg:\n                    date_idxs.add(i)\n                    print(f\"start: {begin_date - time_buffer_itsg}, epoch-begin: {epoch_begin}, UB:{begin_date + time_buffer_itsg}\")\n\n\n            # Add bit more error handling statments \n            # rest is fine -&gt; if inputs's right - output is right\n\n    elif file_type == \"tn-14\":\n        # there will be only one row per month -&gt; for sake of consistency using set\n        # print(\"TN-14 Replacement file\")\n        date_idxs = set()\n        # think of a rather efficient searching scheme\n        time_buffer = timedelta(days=5)\n        time_buffer_itsg = timedelta(days=23)\n        for i in range(rows):\n            begin_date = julian.from_jd(replacemnt_mat[i][-2], fmt='mjd').date()\n            end_date = julian.from_jd(replacemnt_mat[i][-1], fmt='mjd').date()\n\n\n            if epoch_end:\n                if begin_date &gt;= epoch_begin - time_buffer and end_date &lt;= epoch_end + time_buffer:\n                    date_idxs.add(i)\n                    print(f\"start: {begin_date}, epoch-begin: {epoch_begin}, LB:{epoch_begin - time_buffer}, UB: {epoch_end + time_buffer}, end: {end_date}, epoch-end: {epoch_end}\")\n            else:\n                # for itsg\n                if type(epoch_begin) == str:\n                    epoch_begin = datetime.strptime(epoch_begin, \"%Y-%m\").date()\n                if begin_date - time_buffer_itsg &lt;= epoch_begin &lt;= begin_date + time_buffer_itsg:\n                    date_idxs.add(i)\n                    print(f\"start: {begin_date - time_buffer_itsg}, epoch-begin: {epoch_begin}, UB:{begin_date + time_buffer_itsg}\")\n\n                        # Add bit more error handling statments \n            # rest is fine -&gt; if inputs's right - output is right\n\n    else:\n        raise ValueError(\"Technical Note-13 (tn-13) and Technical Note 14 (tn-14) supported...\")\n\n\n    return list(date_idxs)\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def extract_C10_11_replcmnt_coeff(data_tn13, source, epoch_begin, epoch_end=None):\n\n    # match the date\n    file_type = 'tn-13'\n\n    if epoch_end is not None:\n        end_epoch = epoch_end\n    else:\n        end_epoch = None\n\n    if source == 'jpl' or source == 'csr':\n\n        # find the necessary indxes\n        replcmnt_idxs = find_date_in_replacemnt_file(data_tn13, file_type, epoch_begin, end_epoch)\n        # extract the coeff from tn13 for required dates\n\n        C10 = data_tn13[replcmnt_idxs[0], :-2]\n        # extract the coeff from tn13 for required dates\n\n        C11 = data_tn13[replcmnt_idxs[1], :-2]\n\n    elif source == 'itsg':\n\n        replcmnt_idxs = find_date_in_replacemnt_file(data_tn13, file_type, f\"{epoch_begin.year}-{str(epoch_begin.month).zfill(2)}\", end_epoch)\n        # extract the coeff from tn13 for required dates\n\n        C10 = data_tn13[replcmnt_idxs[0], :-2]\n        # extract the coeff from tn13 for required dates\n\n        C11 = data_tn13[replcmnt_idxs[1], :-2]\n\n    else:\n        raise ValueError(\"Invalid Source. The sources recoginized are CSR, ITSG and JPL\")\n\n    return C10, C11\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def extract_C20_replcmnt_coeff(data_tn14, source, epoch_begin, epoch_end=None):\n    # For JPL \n    # generating a CLM array for C20 and C30\n    # NOTE: Zonal coeff. does not have Slm - its taken as 0\n    if source == 'jpl' or source == 'csr':\n\n        replcmnt_idxs  = find_date_in_replacemnt_file(data_tn14, 'tn-14', epoch_begin, epoch_end)\n\n        C20 = np.array([2, 0, data_tn14[replcmnt_idxs[0], 0:2][0], data_tn14[replcmnt_idxs[0], 0:2][1], 0, 0])\n\n    elif source == 'itsg':\n        replcmnt_idxs  = find_date_in_replacemnt_file(data_tn14, 'tn-14', epoch_begin, epoch_end=None)\n\n        C20 = np.array([2, 0, data_tn14[replcmnt_idxs[0], 0:2][0], data_tn14[replcmnt_idxs[0], 0:2][1], 0, 0])\n\n\n    return C20\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def extract_C30_replcmnt_coeff(data_tn14, source, epoch_begin, epoch_end=None):\n    if source == 'jpl' or source == 'csr':\n\n        replcmnt_idxs  = find_date_in_replacemnt_file(data_tn14, 'tn-14', epoch_begin, epoch_end)\n\n        # think about handling nan values while replacing and its impact\n        # handle the nan issue\n\n        C30 = np.array([3, 0, data_tn14[replcmnt_idxs[0], 2:4][0], data_tn14[replcmnt_idxs[0], 2:4][1], 0, 0])\n\n        # replace nan values with zeros\n        C30[np.isnan(C30)] = 0\n\n    elif source == 'itsg':\n        replcmnt_idxs  = find_date_in_replacemnt_file(data_tn14, 'tn-14', epoch_begin, epoch_end=None)\n\n        C30 = np.array([3, 0, data_tn14[replcmnt_idxs[0], 2:4][0], data_tn14[replcmnt_idxs[0], 2:4][1], 0, 0])\n\n        # replace nan values with zeros\n        C30[np.isnan(C30)] = 0\n\n    return C30\n</code></pre> Source code in <code>pyshbundle/io.py</code> <pre><code>def sub2ind(array_shape, rows, cols):\n    # rows, list need to be linear array\n    return rows*array_shape[1] + cols\n</code></pre> <p>Transforms the spherical harmonics coefficients data in clm or klm format into a SC matrix</p> <pre><code>clm data - [l, m, c_lm, s_lm]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>clm_mat</code> <code>ndarray</code> <p>description</p> required <code>lmax</code> <code>int</code> <p>maximum degree of spherical harmonic expansion</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>pyshbundle/io.py</code> <pre><code>def cklm2sc_new(clm_mat, lmax: int):\n    \"\"\"Transforms the spherical harmonics coefficients data in clm or klm format into a SC matrix\n\n        clm data - [l, m, c_lm, s_lm]\n\n    Args:\n        clm_mat (np.ndarray): _description_\n        lmax (int): maximum degree of spherical harmonic expansion\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n\n    # initialise an empty sc matrix\n    sc_mat = np.zeros([lmax+1, 2*lmax + 1])\n    dev_sc_mat = np.zeros([lmax+1, 2*lmax + 1])\n\n    # navigating the maze of indices\n\n    # Use logical indices\n\n    # sc mat requires padding - Taken care of by the earlier initialisation\n    # \n    # filling the value at appropriate locaation is the key\n    # \n    # Approach-1\n        # run through rows(degree) and fill the cols(order) respectively\n\n    # Approach -2\n        # create a row_func s.t. [....., C22, C21, C20, S21, S22, .......]\n        # then stack the rows\n\n    # First flatten the SC matrix - column wise aka Fortran style\n    # get the flattented idx to be raplaced using sub2ind \n    # replace the indices at those locations using \n    # unflatten the matrix\n\n    shape_sc = sc_mat.shape\n\n    # following the approach similar to Octave implementation\n    # using matrix operations to improve the time efficiency as compared to looping\n    idx_s = sub2ind(sc_mat.shape, clm_mat[:, 0].astype('i'), (lmax - clm_mat[:, 1]).astype('i')).astype('i')\n    idx_c = sub2ind(sc_mat.shape, clm_mat[:, 0].astype('i'), (lmax + clm_mat[:, 1]).astype('i')).astype('i')\n\n\n    flat_sc = sc_mat.flatten(\"F\")\n    # Attention first place the slm coeff. or else it will relace zonal clm coeff.\n    flat_sc[idx_s] = clm_mat[:, 3]\n    flat_sc[idx_c] = clm_mat[:, 2]\n\n    flat_sc2 = dev_sc_mat.flatten(\"F\")\n    flat_sc2[idx_s] = clm_mat[:, 5]\n    flat_sc2[idx_c] = clm_mat[:, 4]\n\n    dev_scmat = flat_sc2.reshape(shape_sc)\n\n    scmat = flat_sc.reshape(shape_sc)\n\n    # with one flag include for \n\n    return scmat, dev_scmat\n</code></pre> <p>Extracts the spherical harmonic coefficients from all the given files</p> <p>Currently supports JPL, CSR, and ITSG data sources ONLY. Extracts the spherical harmonic coefficients from the given file and returns them in a dictionary. Uses the degree and order of a coefficient as the key and the coefficient values as the value.</p> <p>Extracts the degree 1 coefficients from the given TN-13 file</p> <p>Ensure the TN-13 file used is the one recommended by respective data centres (JPL, CSR, or ITSG). Similar to extract_SH_data, but specifically for TN-13 files. Returns degree 1 replacement coefficients as a dictionary. Uses the degree and order of a coefficient as the key and the coefficient values as the value.</p> <p>Extracts the degree 2 and 3 coefficients from the given file</p> <p>Ensure the TN-14 file used is the one recommended by respective data centres (JPL, CSR, or ITSG). Similar to extract_SH_data, but specifically for TN-14 files. Returns degree 2, 3 replacement coefficients as a dictionary. Uses the degree and order of a coefficient as the key and the coefficient values as the value.</p> <p>Returns path of data files, path of tn13 and path of tn14 replacement files</p>"},{"location":"load_data/#pyshbundle.io.parse_jpl_file--parameters","title":"Parameters","text":"<p>file_path : str     Absolute path to the file</p> Source code in <code>pyshbundle/io.py</code> <pre><code>def parse_jpl_file(file_path: str):\n    \"\"\"Reads the spherical harmonic data provided by JPL\n\n    Parameters\n    ----------\n    file_path : str\n        Absolute path to the file\n    \"\"\"\n    # ensure that the file path is valid then proceed\n\n    source = 'JPL'\n\n    # check if the file is ziped or not\n\n    if file_path[-3:] == '.gz':\n        # open the file and read the lines\n        with gzip.open(file_path, 'r') as file:\n\n            # read the file line wise -&gt; obtain a list of bytes\n            info_lines = file.readlines()\n            num_lines = len(info_lines)\n\n            for i in range(len(info_lines)):\n                # find the end of header sentence in the text file\n                if str(info_lines[i]) == str(b'# End of YAML header\\n',):\n                    end_of_header_idx = i\n                    break\n\n        # everything after the header is the numerical data    \n        header_info = info_lines[:end_of_header_idx]\n\n        # parse the header strings to extract relavant metadata info\n        jpl_header = parse_jpl_header(header_info)\n\n        # parse the header strings to extract relavant metadata info\n        jpl_data = extract_SH_data(file_path, source='itsg')\n\n    return jpl_header, jpl_data\n</code></pre>"},{"location":"load_data/#pyshbundle.io.extract_SH_data--parameters","title":"Parameters","text":"<p>file_path : str     Absolute path to the file source : str     Source of the data (JPL, CSR, or ITSG)</p>"},{"location":"load_data/#pyshbundle.io.extract_SH_data--returns","title":"Returns","text":"<p>dict     Dictionary containing the coefficients and time coverage start and end dates</p> Source code in <code>pyshbundle/io.py</code> <pre><code>def extract_SH_data(file_path, source):\n    \"\"\"Extracts the spherical harmonic coefficients from all the given files\n\n    Currently supports JPL, CSR, and ITSG data sources ONLY.\n    Extracts the spherical harmonic coefficients from the given file and returns them in a dictionary.\n    Uses the degree and order of a coefficient as the key and the coefficient values as the value.\n\n    Parameters\n    ----------\n    file_path : str\n        Absolute path to the file\n    source : str\n        Source of the data (JPL, CSR, or ITSG)\n\n    Returns\n    ----------\n    dict\n        Dictionary containing the coefficients and time coverage start and end dates\n    \"\"\"\n    # Initialize an empty dictionary to store the coefficients and dates\n    data = {\n        'coefficients': {},\n        'time_coverage_start': None,\n        'time_coverage_end': None\n    }\n\n\n    # Regular expression pattern to match the lines with coefficients\n    coeff_pattern_csr = re.compile(r'^GRCOF2\\s+(\\d+)\\s+(\\d+)\\s+([-+]?\\d*\\.\\d+E[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+E[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+E[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+E[-+]?\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+')\n    coeff_pattern_jpl = re.compile(r'^GRCOF2\\s+(\\d+)\\s+(\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)')\n    coeff_pattern_itsg = re.compile(r'^gfc\\s+(\\d+)\\s+(\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)$')\n\n\n    if source=='jpl': coeff_pattern=coeff_pattern_jpl\n    elif source=='csr': coeff_pattern=coeff_pattern_csr\n    elif source=='itsg': coeff_pattern=coeff_pattern_itsg\n    else: raise ValueError(\"Invalid source, pyshbundle only supports JPL, CSR, and ITSG\")\n\n\n    # Regular expression patterns to match the time coverage start and end lines\n    start_pattern = re.compile(r'time_coverage_start\\s*:\\s*([\\d\\-T:.]+)')\n    end_pattern = re.compile(r'time_coverage_end\\s*:\\s*([\\d\\-T:.]+)')\n    timeindex_itsg = re.compile(r'^modelname\\s+(.+)$')\n\n\n    # Open and read the gzipped file to extract the time coverage start and end dates\n    if source=='itsg':\n        with open(file_path, 'rt') as file:\n            for line in file:\n                # Strip any leading/trailing whitespace characters\n                line = line.strip()\n\n                # Search for time coverage start\n                start_match = timeindex_itsg.search(line)\n                if start_match:\n                    data['time_coverage_start'] = start_match.group(1)\n\n                # Break the loop if both dates are found\n                if data['time_coverage_start']:\n                    break\n            # File is automatically closed here due to the 'with' statement\n        with open(file_path, 'rt') as file:\n            for line in file:\n                # Strip any leading/trailing whitespace characters\n                line = line.strip()\n                # print(line)\n\n                # Search for the coefficient pattern in the line\n                coeff_match = coeff_pattern.search(line)\n                if coeff_match:\n                    # Extract degree, order, Clm, and Slm\n                    degree = int(coeff_match.group(1))\n                    order = int(coeff_match.group(2))\n                    clm = np.double(coeff_match.group(3))\n                    slm = np.double(coeff_match.group(4))\n                    clm_sdev = np.double(coeff_match.group(5))\n                    slm_sdev = np.double(coeff_match.group(6))\n\n                    # Store the coefficients in the dictionary\n                    data['coefficients'][(degree, order)] = {'Clm': clm, 'Slm': slm,\n                                                            'Clm_sdev': clm_sdev, 'Slm_sdev': slm_sdev}\n\n\n\n    elif source=='csr' or source=='jpl':\n        with gzip.open(file_path, 'rt') as file:   # gzip.open\n            for line in file:\n                # Strip any leading/trailing whitespace characters\n                line = line.strip()\n\n                # Search for time coverage start\n                start_match = start_pattern.search(line)\n                if start_match:\n                    data['time_coverage_start'] = start_match.group(1)\n\n                # Search for time coverage end\n                end_match = end_pattern.search(line)\n                if end_match:\n                    data['time_coverage_end'] = end_match.group(1)\n\n                # Break the loop if both dates are found\n                if data['time_coverage_start'] and data['time_coverage_end']:\n                    break\n            # File is automatically closed here due to the 'with' statement\n\n\n        # Open and read the gzipped file again to extract the coefficients\n        with gzip.open(file_path, 'rt') as file:\n            for line in file:\n                # Strip any leading/trailing whitespace characters\n                line = line.strip()\n                # print(line)\n\n                # Search for the coefficient pattern in the line\n                coeff_match = coeff_pattern.search(line)\n                if coeff_match:\n                    # Extract degree, order, Clm, and Slm\n                    degree = int(coeff_match.group(1))\n                    order = int(coeff_match.group(2))\n                    clm = np.double(coeff_match.group(3))\n                    slm = np.double(coeff_match.group(4))\n                    clm_sdev = np.double(coeff_match.group(5))\n                    slm_sdev = np.double(coeff_match.group(6))\n\n                    # Store the coefficients in the dictionary\n                    data['coefficients'][(degree, order)] = {'Clm': clm, 'Slm': slm,\n                                                            'Clm_sdev': clm_sdev, 'Slm_sdev': slm_sdev}\n    return data\n</code></pre>"},{"location":"load_data/#pyshbundle.io.extract_deg1_coeff_tn13--parameters","title":"Parameters","text":"<p>file_path : str     Absolute path to the file</p>"},{"location":"load_data/#pyshbundle.io.extract_deg1_coeff_tn13--returns","title":"Returns","text":"<p>dict     Dictionary containing the degree 1 (order 1) coefficients and time coverage start and end dates</p> Source code in <code>pyshbundle/io.py</code> <pre><code>def extract_deg1_coeff_tn13(file_path):\n    \"\"\"Extracts the degree 1 coefficients from the given TN-13 file\n\n    Ensure the TN-13 file used is the one recommended by respective data centres (JPL, CSR, or ITSG).\n    Similar to extract_SH_data, but specifically for TN-13 files.\n    Returns degree 1 replacement coefficients as a dictionary.\n    Uses the degree and order of a coefficient as the key and the coefficient values as the value.\n\n    Parameters\n    ----------\n    file_path : str\n        Absolute path to the file\n\n    Returns\n    ----------\n    dict\n        Dictionary containing the degree 1 (order 1) coefficients and time coverage start and end dates\n    \"\"\"\n\n    data_dict = {}\n\n    with open(file_path, 'rt') as file:\n        lines = file.readlines()\n        for line in lines:\n\n            # Extract data using regex\n            pattern = re.compile(r'^GRCOF2\\s+(\\d+)\\s+(\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+([-+]?\\d*\\.\\d+e[-+]?\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)')\n            match = pattern.match(line)\n\n            if match:\n                degree = int(match.group(1))\n                order = int(match.group(2))\n                Clm = float(match.group(3))\n                Slm = float(match.group(4))\n                Clm_sdev = np.double(match.group(5))\n                Slm_sdev = np.double(match.group(6))\n                epoch_begin = match.group(7)\n                epoch_end = match.group(8)\n\n                # Use epoch start as key but in yyyy-mm-dd format\n                epoch_key=datetime.strptime(epoch_begin, '%Y%m%d.%H%M%S').strftime('%Y-%m')\n                data_dict[epoch_key, degree, order] = {\n                    'degree': degree,\n                    'order': order,\n                    'Clm': Clm,\n                    'Slm': Slm,\n                    'Clm_sdev': Clm_sdev,\n                    'Slm_sdev': Slm_sdev,\n                    'epoch_begin': epoch_begin,\n                    'epoch_end': epoch_end,\n                }\n    # Print a sample of the data to check if it's parsed correctly\n    # for key in sorted(data_dict.keys())[:5]:  # print first 5 entries\n    #     print(f\"{key}: {data_dict[key]}\")\n    return data_dict\n</code></pre>"},{"location":"load_data/#pyshbundle.io.extract_deg2_3_coeff_tn14--parameters","title":"Parameters","text":"<p>file_path : str     Absolute path to the file</p>"},{"location":"load_data/#pyshbundle.io.extract_deg2_3_coeff_tn14--returns","title":"Returns","text":"<p>dict     Dictionary containing the degree 2,3 (order 0) coefficients and time coverage start and end dates</p> Source code in <code>pyshbundle/io.py</code> <pre><code>def extract_deg2_3_coeff_tn14(file_path):\n    \"\"\"Extracts the degree 2 and 3 coefficients from the given file\n\n    Ensure the TN-14 file used is the one recommended by respective data centres (JPL, CSR, or ITSG).\n    Similar to extract_SH_data, but specifically for TN-14 files.\n    Returns degree 2, 3 replacement coefficients as a dictionary.\n    Uses the degree and order of a coefficient as the key and the coefficient values as the value.\n\n    Parameters\n    ----------\n    file_path : str\n        Absolute path to the file\n\n    Returns\n    ----------\n    dict\n        Dictionary containing the degree 2,3 (order 0) coefficients and time coverage start and end dates\n    \"\"\"\n    data_dict = {}\n\n    with open(file_path, 'rt') as file:\n        lines = file.readlines()\n        for line in lines:\n\n            # Extract data using regex\n            pattern = re.compile(\n                r'(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+([-\\d.eE+]+)\\s+([-\\d.eE+]+)\\s+([-\\d.eE+]+)\\s+([-\\d.eE+]+|NaN)?\\s+([-\\d.eE+]+|NaN)?\\s+([-\\d.eE+]+|NaN)?\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)')\n            match = pattern.match(line)\n\n            if match:\n                mjd_start = float(match.group(1))\n                year_frac_start = float(match.group(2))\n                c20 = np.double(match.group(3))\n                c20_mean_diff = np.double(match.group(4))\n                c20_sigma = np.double(match.group(5))\n                c30 = match.group(6)\n                c30_mean_diff = match.group(7)\n                c30_sigma = match.group(8)\n                mjd_end = float(match.group(9))\n                year_frac_end = float(match.group(10))\n\n                # Only add C30 if it exists (not)\n                if c30.lower() != 'nan':\n                    c30 = np.double(c30)\n                    c30_mean_diff = np.double(c30_mean_diff)\n                    c30_sigma = np.double(c30_sigma)\n                else:\n                    c30 = None\n                    c30_mean_diff = None\n                    c30_sigma = None\n\n                # Use mjd as key but in yyyy-mm-dd format\n                mjd_key = julian.from_jd(mjd_start, fmt='mjd').date().strftime('%Y-%m')\n                data_dict[mjd_key] = {\n                    'year_frac_start': year_frac_start,\n                    'mjd_start': mjd_start,\n                    'c20': c20,\n                    'c20_mean_diff': c20_mean_diff,\n                    'c20_sigma': c20_sigma,\n                    'c30': c30,\n                    'c30_mean_diff': c30_mean_diff,\n                    'c30_sigma': c30_sigma,\n                    'mjd_end': mjd_end,\n                    'year_frac_end': year_frac_end\n                }\n    # Print a sample of the data to check if it's parsed correctly\n    # for key in sorted(data_dict.keys())[:5]:  # print first 5 entries\n    #     print(f\"{key}: {data_dict[key]}\")\n    return data_dict\n</code></pre>"},{"location":"load_data/#pyshbundle.io.read_GRACE_SH_paths--parameters","title":"Parameters","text":"<p>use_sample_files : (int, optional)     Defaults to 0.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>description</p>"},{"location":"load_data/#pyshbundle.io.read_GRACE_SH_paths--returns","title":"Returns","text":"<p>path_sh, path_tn13, path_tn14, source : str     Path of data files, path of tn13 and path of tn14 replacement files,      source of the SH files (JPL, ITSG or CSR)</p> Remarks <p>The purpose of this script is to, firstly read what the data source is (JPL, CSR or ITSG) read file path for GRACE L2 spherical harmonics inputs, read replacement files for tn13 and tn14 source of the SH files (JPL, ITSG or CSR)</p> Source code in <code>pyshbundle/io.py</code> <pre><code>def read_GRACE_SH_paths(use_sample_files = 0):\n\n    \"\"\"Returns path of data files, path of tn13 and path of tn14 replacement files\n\n    Parameters\n    ----------\n    use_sample_files : (int, optional)\n        Defaults to 0.\n\n    Raises:\n        Exception: _description_\n\n    Returns\n    ----------\n    path_sh, path_tn13, path_tn14, source : str\n        Path of data files, path of tn13 and path of tn14 replacement files, \n        source of the SH files (JPL, ITSG or CSR)\n\n    Remarks:\n        The purpose of this script is to,\n        firstly read what the data source is (JPL, CSR or ITSG)\n        read file path for GRACE L2 spherical harmonics inputs,\n        read replacement files for tn13 and tn14\n        source of the SH files (JPL, ITSG or CSR)\n    \"\"\"\n    #Created on Fri Feb  17 2023\n    #@author: Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n\n    print(\"This program supports working with GRACE L2 Spherical harmonics data from the following centers: CSR, JPL and ITSG\")\n    print(\"Instructions to download data may be referred to in https://github.com/mn5hk/pyshbundle/blob/main/docs/index.md#how-to-download-data\")\n    source = str(input(\"Enter the source of L2 SH coeffs code(jpl, csr, gfz): \"))\n\n    if use_sample_files ==1:\n\n        print(\"You have chosen to use sample replacement files.\")\n        print(\"The replacement files for the TN13 and TN14 Args have been preloaded into the program\")\n        print(\"Due to the size of the GRACE SH files, these have not been preloaded into the program\")\n        print(\"You may download the GRACE SH L2 files from the link below. Please ensure to download the files as per your selection of source in the prior step\")\n        print(\"Download sample files from: https://github.com/mn5hk/pyshbundle/tree/main/sample_input_data\")\n    path_sh = str(input(\"Enter the path to the folder with SH L2 data\"))\n\n\n    if str.upper(source) == 'JPL':\n        if use_sample_files == 1:\n            path_tn13 = pkg_resources.resource_filename('pyshbundle', 'data/sample_JPL_TN_files/TN-13_GEOC_JPL_RL06.txt')\n            path_tn14 = pkg_resources.resource_filename('pyshbundle', 'data/sample_JPL_TN_files/TN-14_C30_C20_GSFC_SLR.txt')\n            print(\"Successfully loaded preloaded TN13 and TN14 replacement files for JPL\")\n        else:\n            path_tn13 = str(input(\"Enter the path to the file for tn13 replacement in .txt format\"))\n            path_tn14 = str(input(\"Enter the path to the file for tn14 replacement in .txt format\"))\n            print(\"Successfully loaded TN13 and TN14 replacement files for JPL\")\n\n    elif str.upper(source) == 'CSR':\n        if use_sample_files == 1:\n            path_tn13 = pkg_resources.resource_filename('pyshbundle', 'data/sample_CSR_TN_files/TN-14_C30_C20_SLR_GSFC.txt')\n            path_tn14 = pkg_resources.resource_filename('pyshbundle', 'data/sample_CSR_TN_files/TN-13_GEOC_CSR_RL06.1.txt')\n            print(\"Successfully loaded preloaded TN13 and TN14 replacement files for CSR\")\n        else:\n            path_tn13 = str(input(\"Enter the path to the file for tn13 replacement in .txt format\"))\n            path_tn14 = str(input(\"Enter the path to the file for tn14 replacement in .txt format\"))\n            print(\"Successfully loaded TN13 and TN14 replacement files for CSR\")\n\n    elif str.upper(source) == 'ITSG':\n        if use_sample_files == 1:\n            path_tn13 = pkg_resources.resource_filename('pyshbundle', 'data/sample_ITSG_TN_files/TN-13_GEOC_CSR_RL06.1.txt')\n            path_tn14 = pkg_resources.resource_filename('pyshbundle', 'data/sample_ITSG_TN_files/TN-14_C30_C20_SLR_GSFC.txt')\n            print(\"Successfully loaded preloaded TN13 and TN14 replacement files for ITSG\")\n        else:\n            path_tn13 = str(input(\"Enter the path to the file for tn13 replacement in .txt format\"))\n            path_tn14 = str(input(\"Enter the path to the file for tn14 replacement in .txt format\"))\n            print(\"Successfully loaded TN13 and TN14 replacement files for ITSG\")\n    else:\n        raise Exception(\"Source selection is incorrect. Please select between JPL, CSR or gfz\")\n\n    return path_sh, path_tn13, path_tn14, source\n</code></pre>"},{"location":"load_data/#computing-ling-term-mean","title":"Computing Ling Term Mean","text":"<p>Loads the long term mean values for the GRACE SH data</p>"},{"location":"load_data/#pyshbundle.io.load_longterm_mean--parameters","title":"Parameters","text":"<p>source (str, optional):      Source of data. Defaults to \"\". use_sample_mean (int, optional)     Whether to use default long-mean values provided with the data. Defaults to 0.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>description</p>"},{"location":"load_data/#pyshbundle.io.load_longterm_mean--returns","title":"Returns","text":"<p>long_mean : str     path of the appropriate long term mean file</p> Todo <ul> <li>Not sure if using \"source = ''\" is all right</li> <li>instead of base eception is can be ValueError</li> </ul> Source code in <code>pyshbundle/io.py</code> <pre><code>def load_longterm_mean(source = \"\", use_sample_mean = 0):\n    \"\"\"Loads the long term mean values for the GRACE SH data\n\n    Parameters\n    ----------\n    source (str, optional): \n        Source of data. Defaults to \"\".\n    use_sample_mean (int, optional)\n        Whether to use default long-mean values provided with the data. Defaults to 0.\n\n    Raises:\n        Exception: _description_\n\n    Returns\n    ----------\n    long_mean : str\n        path of the appropriate long term mean file\n\n    Todo:\n        + Not sure if using \"source = ''\" is all right\n        + instead of base eception is can be ValueError\n    \"\"\"\n    # @author: Amin Shakya, Interdisciplinary Center for Water Research (ICWaR), Indian Institute of Science (IISc)\n    \"\"\"The purpose of this script is to,\n        load longterm mean for our GRACE SH data\n\n        For this, we need to input the GRACE data source as well as the path to the longterm mean values\n        Data source may be CSR, JPL or ITSG.\n\n        For RL06, example data have been provided within the package. In case this option is chosen, the program directly returns the longterm mean values.\n\n        Returns the long_mean path\n    \"\"\"\n\n    if use_sample_mean == 1:\n        print(\"Loading preloaded RL06 long term mean values\")\n        print(\"Please ensure that your data is RL06 \\nIf not, please manually input long term mean by setting use_sample_mean = 0\")\n\n        if str.upper(source) == 'CSR':\n            long_mean = pkg_resources.resource_filename('pyshbundle', 'data/RL06_long_mean/SH_long_mean_csr.npy')\n        elif str.upper(source) == 'JPL':\n            long_mean = pkg_resources.resource_filename('pyshbundle', 'data/RL06_long_mean/SH_long_mean_itsg.npy')\n        elif str.upper(source) == 'ITSG':\n            long_mean = pkg_resources.resource_filename('pyshbundle', 'data/RL06_long_mean/SH_long_mean_jpl.npy')\n        else:\n            raise Exception(\"Incorrect selection of source\")\n        print(\"Successfully loaded preloaded longterm means\")\n    else:\n        print(\"Please download and provide the longterm GRACE SH mean values\")\n        print(\"Instructions to download the longterm GRACE SH mean values may be referred to in https://github.com/mn5hk/pyshbundle/blob/main/docs/index.md#how-to-download-data\")\n        long_mean = str(input(\"Enter the longterm mean for the SH values in the numpy (.npy) format\"))\n        print(\"Successfully loaded path to long term mean:\", long_mean)\n\n    return long_mean\n</code></pre>"},{"location":"load_data/#physical-and-geodetic-constants","title":"Physical and Geodetic Constants","text":"<p>This script contains some of the major relavant Physical and Geodetic(GRS80) constants:</p> <ul> <li><code>clight</code> speed of light - \\(2.99792458e+8\\) \\(m/s\\)</li> <li><code>G</code> Gravitational constant- \\(6.67259e-11\\) $\frac{m^3} {kg \\cdot s^2}$</li> <li> <p><code>au</code> astronomical unit - \\(149.597870691e+9\\) \\(m\\)</p> </li> <li> <p><code>ae</code> semi-major axis of ellipsoid <code>GRS 80</code>- \\(6378137\\) m</p> </li> <li><code>GM</code> geocentric grav. constant <code>GRS 80</code>- \\(3.986005e+14\\) $\frac{m^3}{s^2}$</li> <li><code>J2</code> earth's dynamic form factor <code>GRS 80</code> - \\(1.08263e-3\\) [unitless C20 unnormalized coefficient]</li> <li> <p><code>Omega</code> mean ang. velocity <code>GRS 80</code> - $7.292115e-5 $\frac{rad}{s}$</p> </li> <li> <p><code>flat</code> flattening - $\frac{1}{298.257222101}$</p> </li> </ul>"},{"location":"load_data/#reference","title":"Reference","text":""},{"location":"pyshbundle/","title":"Reference Mannual - PySHBundle","text":"<p>The module codes can be categorized into four categories:</p> <ul> <li>Load Data Modules</li> <li>convert data formats</li> <li>core functionality</li> <li>auxillary codes</li> </ul> <p></p> <p>Navigate the Reference Manual based on the following schematic</p>"},{"location":"theory/","title":"Theoretical Background","text":""},{"location":"theory/#mathematics","title":"Mathematics","text":"<p>In this section, we present a mathematical representation of the spherical harmonics analysis. According to potential theory, the gravitational field of a body fulfils the Laplace equation \\(\\nabla^2\\phi = 0\\). Laplace's equation in spherical coordinates can be written as follows: </p> \\[\\begin{equation}     \\frac{1}{r^2}\\frac{\\partial}{\\partial r}\\bigg( r^2\\frac{\\partial \\phi}{\\partial r}\\bigg)       +     \\frac{1}{r^2\\sin\\vartheta}\\frac{\\partial}{\\partial \\vartheta}\\bigg(\\sin\\vartheta\\frac{\\partial \\phi}{\\partial \\vartheta}\\bigg)      +     \\frac{1}{r^2\\sin^2\\vartheta}\\frac{\\partial^2 \\phi}{\\partial \\lambda^2}     = 0 , \\end{equation}\\] <p>where  \\(\\phi\\) is the potential,  \\(r\\) is the radius,  \\(\\vartheta\\) is the co-latitude and  \\(\\lambda\\) is the longitude. </p> <p>We perform a separation of variables and insert \\(\\phi(r, \\vartheta, \\lambda) =f(r)g(\\vartheta)h(\\lambda)\\) into the Laplace equation to get three independent equations:</p> \\[\\begin{equation} r^2\\frac{d^2f}{dr^2}+2r\\frac{df}{dr} - n(n+1)f = 0, \\end{equation}\\] \\[\\begin{equation} \\frac{d^2g}{d\\vartheta^2} + \\frac{dg}{d\\vartheta}\\cot\\vartheta + \\bigg(  n(n+1) - \\frac{m^2}{\\sin^2\\vartheta}   \\bigg) g = 0 , \\end{equation}\\] \\[\\begin{equation} \\frac{d^2h}{d\\lambda^2} + m^2h = 0, \\end{equation}\\] <p>where \\(m\\) and \\(n\\) are the degree and order respectively. Solving \\((2), (3)\\) and \\((4)\\), we obtain: </p> \\[\\begin{equation} f(r) \\in \\{r^n, r^{-(n+1)}\\}, \\end{equation}\\] \\[\\begin{equation} g(\\vartheta) \\in \\{P_{n,m}(\\cos \\vartheta), Q_{n,m}(\\cos \\vartheta)\\} , \\end{equation}\\] <p>\\begin{equation} h(\\lambda) \\in {\\cos m\\lambda, \\sin m\\lambda}. \\end{equation}\\</p> <p>Thus, the Laplace equation's solution takes the following form: </p> \\[\\begin{equation} \\phi(r, \\vartheta, \\lambda) = \\sum_{n=0}^{\\infty} \\sum_{m=0}^{n}  \\alpha_{n,m} \\begin{Bmatrix} P_{n,m}(\\cos\\vartheta)\\\\ Q_{n,m}(\\cos\\vartheta)\\\\ \\end{Bmatrix} \\dot{\u2022} \\begin{Bmatrix} \\cos m\\lambda\\\\ \\sin m\\lambda\\\\ \\end{Bmatrix} \\dot{\u2022} \\begin{Bmatrix} r^n\\\\ r^{(n+1)}\\\\ \\end{Bmatrix} . \\end{equation}\\] <p>Solutions for \\(f(r)\\) and \\(h(\\lambda)\\) are fairly straightforward. Eq - (3) for \\(g(\\vartheta)\\) is in the form of a Legendre differential equation and its solutions are \\(P_{n,m}(\\cos \\vartheta)\\) and \\(Q_{n,m}(\\cos \\vartheta)\\), the associated Legendre functions of the first and second kind. We now apply two constraints to the solution:</p> <ul> <li>\\(\\phi \\rightarrow 0\\) when \\(r \\rightarrow \\infty\\),</li> <li>\\(\\phi\\) is limited on the sphere,</li> </ul> <p>which leads us to eliminate \\(Q_{n,m}(\\cos \\vartheta)\\) and \\(r^n\\).The \\(4\\pi\\) normalization of the Associated Legendre functions [8] is utilized in our package and is given by: </p> \\[\\begin{equation} \\bar{P}_{n,m}(\\cos\\vartheta) = P_{n,m}(\\cos\\vartheta)\\sqrt{(2-\\delta_{m0})(2n+1)\\frac{(n-m)!}{(n+m)!}}, \\end{equation}\\] <p>where \\(\\delta_{m0}\\) is the Kronecker delta function,</p> \\[\\begin{equation} P_{n,m}(t) = (1-t^2)^{\\frac{m}{2}}\\frac{d^mP_n(t)}{dt^m}, \\end{equation}\\] <p>and </p> \\[\\begin{equation} nP_n(t)=-(n-1)P_{n-2}(t) + (2n-1)tP_{n-1}(t). \\end{equation}\\] <p>Spherical harmonics are the angular portion of a set of solutions to Laplace's equation. They take into account \\(\\vartheta\\) and \\(\\lambda\\). They are functions modelled on the surface of a sphere, denoted by \\(Y_{n,m}(\\vartheta,\\lambda)\\). They are of three kinds: </p> <ul> <li>Zonal harmonics: \\(m=0\\) - they are only latitude dependent,</li> <li>Tesseral harmonics: \\(0 &lt; m &lt; n\\), and </li> <li>Sectorial harmonics: \\(m=n\\).</li> </ul> <p>Quantities like the gravitational potential, height of water column, gravity anomaly and so on are the functionals of the gravity field which are obtained by differentiating the potential \\(\\phi\\) with respect to the spherical coordinates. </p> <p>The gravitational potential anomaly \\(V\\) is given by:</p> \\[\\begin{equation}     V(r, \\vartheta, \\lambda) =      \\frac{GM}{r} \\sum_{n=0} ^{N_{max}} \\sum_{m=0} ^{n}      \\left(\\frac{R}{r}\\right) ^{n+1}     \\bar{P}_{n,m}(\\cos \\vartheta) [C_{n,m}\\cos m\\lambda+S_{n,m}\\sin m\\lambda]. \\end{equation}\\] <p>Here, \\(R\\) refers to the radius of the Earth, \\(\\bar{P}_ {n,m}\\) refers to the Associated Legendre functions with \\(4\\pi\\) normalization, \\(C_{lm}\\) and  \\(S_{lm}\\) refer to the spherical harmonic coefficients. Similarly, another functional, the change in surface mass density, is represented by:</p> \\[\\begin{equation}     \\Delta\\sigma(\\vartheta, \\lambda) =      \\frac{a\\rho_{ave}}{3}      \\sum_{n=0}^{N_{max}}\\sum_{m=0}^{n}      \\left(\\frac{R}{r}\\right)^{n+1}      \\bar{P}_{n,m}(\\cos\\vartheta)     \\frac{2n+1}{1+k_l}     [C_{n,m}\\cos m\\lambda + S_{n,m}\\sin m\\lambda], \\end{equation}\\] <p>where \\(\\rho_{ave}\\) refers to the average density of the Earth in \\(g/cm^3\\) and \\(k_n\\) refers to the load Love number of degree \\(n\\).</p>"},{"location":"theory/#grace-data-levels","title":"GRACE Data Levels","text":"<p>The <code>GRACE</code> data products are being developed, processed  and archieved in a shared Science Data System between the <code>Jet Propulsion Laboratory(JPL)</code>, the <code>University of Texas Center for Space Research (UT-CSR)</code> and <code>GeoForschungsZentrum Potsdam (GFZ)</code>.</p> <ul> <li> <p>Level 0:     The level-0 data are the result of the data reception, collection and decommutation by the Raw Data Center (RDC) of the Mission Operation System (MOS) located in Neustrelitz, Germany. The MOS receives twice per day using its Weilheim and Neustrelitz tracking antennae the science instrument and housekeeping data from each GRACE satellite which will be stored in two appropriate files in the level-0 rolling archive at DFD/Neustrelitz. The SDS retrieves these files and extracts and reformats the orresponding instrument and ancillary housekeeping data like GPS navigation solutions,space segment temperatures or thruster firing events. Level-0 products are available 24-hours after data reception.</p> </li> <li> <p>Level 1:     The level-1 data are the preprocessed, time-tagged and normal-pointed instrument data. These are the K-band ranging, accelerometer, star camera and GPS data of both satellites. Additionally the preliminary orbits of both GRACE satellites will be generated. Level-1 data processing software is developed by JPL with support from GFZ (e.g. accelerometer data preprocessing). Processing of level-1 products is done primarily at JPL. An identical processing system (hardware/software) is installed at GFZ to serve as a backup system in case of hardware or network problems. This double implementation is necessary to guarantee the envisaged level-1 product delay of 5 days. All level-1 products are archived at JPL\u2019s Physical Oceanography Distributed Active Data Center(PODAAC) and at GFZ\u2019s Integrated System Data Center (ISDC) . Both archives are harmonized on a sub-daily timeframe.</p> </li> <li> <p>Level 2: <code>Spherical Harmonic Coefficients</code> for the geopotential</p> <p>Level-2 data include the short term (30 days) and mean gravity field derived from calibrated and validated GRACE level-1 data products. This level also includes ancillary data sets (temperature and pressure fields, ocean bottom pressure, and hydrological data) which are necessary to eliminate time variabilities in gravity field solutions. Additionally the precise orbits of both GRACE satellites are generated. All level-2 products are archived at JPL\u2019s PODAAC and at GFZs ISDC and are available 60 days after data taking. The level-2 processing software were developed independently by all three processing centres using already existing but completely independent software packages which were upgraded for GRACE specific tasks. Common data file interfaces guarantees a strong product validation. Routine processing is done at UTCSR and GFZ, while JPL only generate level-2 products at times for verification purposes.</p> </li> <li> <p>Level 3: <code>Mascons</code>     consists of mass anomalies or other standardized products such as Monthly Ocean/Land Water Equivalent Thickness, Surface-Mass Anomaly. Similarly mass concentration blocks or <code>mascons</code> are also available.</p> </li> <li> <p>Level 4: <code>Time Series</code>     Time-series of catchment level hydrological estimates of TWSA</p> </li> </ul> <p><code>PySHBundle</code> provides the capability to obtain grided Total Water Storage Anomaly(TWSA) from Level 2 data.</p> <p>Spherical harmonic functions or coefficients, Legendre functions and their derivatives can be arranged in different ways. There are multiple functions in SHBundle for reordering from one format to another. Some of them have been translated to Python in PySHBundle. Couple of new ones have also been added.</p>"},{"location":"theory/#spherical-harmonics-data-formats","title":"Spherical Harmonics Data Formats","text":""},{"location":"theory/#clm-format","title":"clm-format","text":"<p>This is a standard format to store spherical harmonic coefficients in the indexed column-vector-format (abbreviatedL clm-format)</p> \\[\\begin{equation}   \\left( n, m, \\overline{C}_{n, m}, \\overline{S}_{n, m}, \\left[ \\sigma_{\\overline{C}_{n, m}}, \\sigma_{\\overline{S}_{n, m}} \\right] \\right) \\end{equation}\\] <p>The first column represents the degree \\(n\\), the second column represents the order \\(m\\) (both n,m are integers), followed by the coefficients \\(\\overline{C}_{n, m}, \\overline{S}_{n, m}\\) and the last two columns contain their respective standard deviations \\(\\sigma_{\\overline{C}_{n, m}}, \\sigma_{\\overline{S}_{n, m}}\\)</p>"},{"location":"theory/#klm-format","title":"klm-format","text":"<p>This is a variation of the clm-format for compact notation with just 3 or 4 columns. The coefficients are sorted first w.r.t. degree and then the order, particularly the sine-coefficients are arranged starting first with negative orders. The following matrix represents the klm-format:</p> \\[\\begin{bmatrix}     0 &amp; 0 &amp; \\overline{C}_{0, 0} &amp; \\sigma_{\\overline{C}_{0, 0}} \\\\     0 &amp; 0 &amp; \\overline{C}_{0, 0} &amp; \\sigma_{\\overline{C}_{0, 0}} \\\\     0 &amp; 0 &amp; \\overline{C}_{0, 0} &amp; \\sigma_{\\overline{C}_{0, 0}} \\\\      0 &amp; 0 &amp; \\overline{C}_{0, 0} &amp; \\sigma_{\\overline{C}_{0, 0}} \\\\      0 &amp; 0 &amp; \\overline{C}_{0, 0} &amp; \\sigma_{\\overline{C}_{0, 0}} \\\\      0 &amp; 0 &amp; \\overline{C}_{0, 0} &amp; \\sigma_{\\overline{C}_{0, 0}} \\\\      0 &amp; 0 &amp; \\overline{C}_{0, 0} &amp; \\sigma_{\\overline{C}_{0, 0}} \\\\      0 &amp; 0 &amp; \\overline{C}_{0, 0} &amp; \\sigma_{\\overline{C}_{0, 0}} \\\\     0 &amp; 0 &amp; \\overline{C}_{0, 0} &amp; \\sigma_{\\overline{C}_{0, 0}} \\\\      \\vdots &amp; &amp; &amp; \\vdots \\\\ N_{max} &amp; N_{max} &amp; \\overline{C}_{N_{max}, N_{max}} &amp; \\sigma_{\\overline{C}_{N_max}, N_{max}} \\\\  \\end{bmatrix}\\]"},{"location":"theory/#left-c-backslash-s-right-format","title":"\\(\\left | C \\backslash S \\right |\\) format","text":"<p>This is another well known arrangement of Spherical Harmonic coefficients. This is a square matrix of size \\(n_{max}, n_{max}\\).</p> <p>The lower traingular terms are made of the cosine terms</p>"},{"location":"theory/#left-s-c-right-backslash-format","title":"\\(\\left / S | C \\right \\backslash\\) format","text":"<p>This is yet another popular format where the sine-coefficients are flipped from left to right, to obtain a triangular arrangement which is completed by zeros.</p> <p>The following figure illustrates the  \\(\\left | C \\backslash S \\right |\\) and  \\(\\left / S | C \\right \\backslash\\) format respectively.</p> <p></p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>In order to make getting started with PySHBundle simple, we have curated a set of introductory tutorials in form of jupyter notebooks. Data for trying out this new tool is included in the repo. After installing and cloning the repo, go to the notebooks directory in order to find explainatory ipython jupyter notebooks. Simply activate the virtual environment and fire up these jupyter notebooks. The following notebooks explain the usage of the tool aswell as some crucial basics of Spherical Harmonics processing for Grace.</p> <ol> <li>Introduction to Spherical Harmonics</li> <li>Loading the data</li> <li>Visualizations</li> <li>Terrestrial Water Storage (TWS) Time Series</li> <li>Tests and Validation notebook</li> </ol>"}]}